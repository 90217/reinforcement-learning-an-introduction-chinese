第11章 \*离策略近似方法
===============================================

自第5章以来，本书主要将处理在策略和离策略学习方法作为处理广义策略迭代学习形式中固有的利用与探索之间冲突的两种替代方法。
前面的两章已经用函数近似处理了在策略情况，在本章中我们用函数近似来处理离策略情况。
与在策略学习相比，函数近似的扩展与离策略学习相比显着不同且更难。
第6章和第7章中提出的表格式离策略方法很容易扩展到半梯度算法，但这些算法并没有像在策略训练下那样强大地收敛。
在本章中，我们将探讨收敛问题，仔细研究线性函数近似理论，引入可学性概念，然后讨论具有更强收敛性保证的新算法。
最后，我们将采用改进的方法，但理论结果不会像在策略学习那样强大，也不会像实验结果一样令人满意。
在此过程中，我们将更深入地了解强化学习中的近似值，以用于在策略学习和离策略学习。

回想一下，在离策略学习中，我们寻求学习 *目标策略* :math:`\pi` 的价值函数，给定数据由于不同的 *行为策略* :math:`b`。
在预测案例中，两个策略都是静态的并已知，我们试图学习状态价值 :math:`\hat{v} \approx v_{\pi}`
或动作值 :math:`\hat{q} \approx q_{\pi}`。
在控制案例中，学习动作价值，并且两种策略通常在学习过程中发生变化── :math:`\pi` 是关于 :math:`\hat{q}` 的贪婪策略，
而 :math:`b` 是更具探索性的东西，例如关于 :math:`\hat{q}` 的 :math:`\varepsilon` -贪婪策略。

离策略学习的挑战可以分为两部分，一部分出现在表格案例中，另一部分出现在函数近似中。
挑战的第一部分与更新的目标有关（不要与目标策略混淆），第二部分与更新的分布有关。
第5章和第7章中提出的与重要性采样相关的技术涉及第一部分；
这些可能会增加方差，但在所有成功的算法，不管是表格的还是近似的都需要。
这些技术在函数近似中的扩展将在本章的第一部分中快速讨论。

由于函数近似的离策略学习挑战的第二部分需要更多的东西，因为离策略情况下的更新分布不是根据在策略分布。
在策略分布对半梯度方法的稳定性很重要。已经探索了两种一般方法来解决这个问题。
一种是再次使用重要性抽样方法，这次将更新分布转回到在策略上的分布，以便保证半梯度方法收敛（在线性情况下）。
另一种是开发真正的梯度方法，不依赖于任何特殊的稳定性分布。
我们提出了基于这两种方法的方法。这是一个前沿的研究领域，目前尚不清楚这些方法中哪一个在实践中最有效。


11.1 半梯度方法
---------------


11.2 离策略差异例子
--------------------


11.3 致命的三合会
------------------


11.4 线性价值函数几何
-----------------------


11.5 Bellman误差中的梯度下降
------------------------------


11.6 Bellman误差是不可学习的
------------------------------


11.7 梯度TD方法
-----------------


11.8 强调TD方法
---------------


11.9 减小误差
---------------


11.10 总结
---------------


书目和历史评论
---------------
