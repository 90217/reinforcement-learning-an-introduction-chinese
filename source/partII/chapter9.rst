第9章 在策略预测近似方法
===========================

在本章中，我们开始研究强化学习中的函数近似，考虑其在从在策略数据中估计状态-价值函数的用途，
即从使用已知策略 :math:`\pi` 生成的经验近似 :math:`v_\pi`。
本章的新颖之处在于，近似值函数不是用表格表示，
而表示成是具有权重向量 :math:`\mathbf{w} \in \mathbb{R}^{d}` 的参数化函数形式。
我们将权重向量 :math:`\mathbf{w}` 的状态 :math:`s` 的近似值写作 :math:`\hat{v}(s,\mathbf{w})\approx v_{\pi}(s)`。
例如，:math:`\hat{v}` 可能是状态特征中的线性函数，:math:`\mathbf{w}` 是特征权重的向量。
更一般地，:math:`\hat{v}` 可以是由多层人工神经网络计算的函数，其中 :math:`\mathbf{w}` 是所有层中的连接权重的向量。
通过调整权重，网络可以实现各种不同函数中的任何一种。
或者 :math:`\hat{v}` 可以是由决策树计算的函数，其中 :math:`\mathbf{w}` 是定义树的分裂点和叶值的所有数字。
通常，权重的数量（:math:`\mathbf{w}` 的维数）远小于状态的数量（:math:`d\ll|\mathcal{S}|`），
并且改变一个权重会改变许多状态的估计值。因此，当更新单个状态时，更改会从该状态推广到许多其他状态的值。
这种 *泛化* 使得学习可能更强大，但也可能更难以管理和理解。

也许令人惊讶的是，将强化学习扩展到函数近似也使其适用于部分可观察到的问题，其中个体无法获得完整状态。
如果 :math:`\hat{v}` 的参数化函数形式不允许估计值依赖于状态的某些方面，那么就好像这些方面是不可观察的。
实际上，本书这一部分中使用函数近似的方法的所有理论结果同样适用于部分可观察的情况。
然而，函数近似不能做的是用过去观察的记忆来增强状态表示。第17.3节简要讨论了一些可能的进一步扩展。


9.1 价值函数近似
-------------------

本书中涉及的所有预测方法都被描述为对估计价值函数的更新，该函数将其在特定状态下的值转换为该状态的“备份值”或 *更新目标*。
让我们使用符号 :math:`s \mapsto u` 的表示单独更新，其中 :math:`s` 是更新的状态，
:math:`u` 是 :math:`s` 的估计价值转移到的更新目标。
例如，价值预测的蒙特卡洛更新是 :math:`S_{t} \mapsto G_{t}`，
TD(0)更新是 :math:`S_{t} \mapsto R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbf{w}_{t})`，
n步TD更新为 :math:`S_{t} \mapsto G_{t:t+n}`。在DP（动态规划）中策略评估更新，
:math:`s\mapsto\mathbb{E}_{\pi}\left[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_{t})|S_{t}=s\right]`，
任意状态 :math:`s` 被更新，而在其他情况下，实际经验中遇到的状态 :math:`S_t` 被更新。

将每个更新解释为指定价值函数的所需输入-输出行为的示例是很自然的。
从某种意义上说，更新 :math:`s \mapsto u` 表示状态 :math:`s` 的估计值应更像更新目标 :math:`u`。
到目前为止，实际的更新是微不足道的：:math:`s` 的估计值的表条目已经简单地转移到了 :math:`u` 的一小部分，
并且所有其他状态的估计值保持不变。现在，我们允许任意复杂和精致方法来实现更新，并在 :math:`s` 处进行更新，以便更改许多其他状态的估计值。
学习以这种方式模拟输入输出示例的机器学习方法称为 *监督学习* 方法，当输出是像 :math:`u` 的数字时，该过程通常称为 *函数近似*。
函数近似方法期望接收它们试图近似的函数的期望输入-输出行为的示例。
我们使用这些方法进行价值预测，只需将每次更新的 :math:`s \mapsto u` 作为训练样例传递给它们。
然后，我们将它们产生的近似函数解释为估计价值函数。

以这种方式将每个更新视为传统的训练示例使我们能够使用任何广泛的现有函数近似方法来进行价值预测。
原则上，我们可以使用任何方法进行监督学习，例如人工神经网络，决策树和各种多元回归。
然而，并非所有函数近似方法都同样适用于强化学习。最复杂的人工神经网络和统计方法都假设一个静态训练集，在其上进行多次传递。
然而，在强化学习中，学习能够在线进行，而个体与其环境或其环境模型进行交互是很重要的。
要做到这一点，需要能够从增量获取的数据中有效学习的方法。
此外，强化学习通常需要函数近似方法能够处理非平稳目标函数（随时间变化的目标函数）。
例如，在基于GPI（广义策略迭代）的控制方法中，我们经常寻求在 :math:`\pi` 变化时学习 :math:`q_\pi`。
即使策略保持不变，如果训练示例的目标值是通过自举方法（DP和TD学习）生成的，则它们也是非平稳的。
不能轻易处理这种非平稳性的方法不太适合强化学习。


9.2 预测目标
------------------


9.3 随机梯度和半梯度方法
--------------------------


9.4 线性方法
----------------


9.5 线性方法的特征构造
------------------------

9.5.1 多项式
^^^^^^^^^^^^^^^

9.5.2 傅立叶基
^^^^^^^^^^^^^^^^^

9.5.3 粗编码（Coarse Coding）
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

9.5.4 平铺编码（Tile Coding）
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

9.5.5 径向基函数
^^^^^^^^^^^^^^^^^^^


9.6 手动选择步长参数
-----------------------


9.7 非线性函数近似：人工神经网络
---------------------------------


9.8 最小二乘TD
----------------------


9.9 基于内存的函数近似
-------------------------


9.10 基于核的函数近似
------------------------


9.11 深入研究在策略学习：兴趣和重点
------------------------------------


9.12 总结
--------------


书目和历史评论
---------------