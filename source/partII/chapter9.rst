第9章 在策略预测近似方法
===========================

在本章中，我们开始研究强化学习中的函数近似，考虑其在从在策略数据中估计状态-价值函数的用途，
即从使用已知策略 :math:`\pi` 生成的经验近似 :math:`v_\pi`。
本章的新颖之处在于，近似值函数不是用表格表示，
而表示成是具有权重向量 :math:`\mathbf{w} \in \mathbb{R}^{d}` 的参数化函数形式。
我们将权重向量 :math:`\mathbf{w}` 的状态 :math:`s` 的近似值写作 :math:`\hat{v}(s,\mathbf{w})\approx v_{\pi}(s)`。
例如，:math:`\hat{v}` 可能是状态特征中的线性函数，:math:`\mathbf{w}` 是特征权重的向量。
更一般地，:math:`\hat{v}` 可以是由多层人工神经网络计算的函数，其中 :math:`\mathbf{w}` 是所有层中的连接权重的向量。
通过调整权重，网络可以实现各种不同函数中的任何一种。
或者 :math:`\hat{v}` 可以是由决策树计算的函数，其中 :math:`\mathbf{w}` 是定义树的分裂点和叶值的所有数字。
通常，权重的数量（:math:`\mathbf{w}` 的维数）远小于状态的数量（:math:`d\ll|\mathcal{S}|`），
并且改变一个权重会改变许多状态的估计值。因此，当更新单个状态时，更改会从该状态推广到许多其他状态的值。
这种 *泛化* 使得学习可能更强大，但也可能更难以管理和理解。

也许令人惊讶的是，将强化学习扩展到函数近似也使其适用于部分可观察到的问题，其中个体无法获得完整状态。
如果 :math:`\hat{v}` 的参数化函数形式不允许估计值依赖于状态的某些方面，那么就好像这些方面是不可观察的。
实际上，本书这一部分中使用函数近似的方法的所有理论结果同样适用于部分可观察的情况。
然而，函数近似不能做的是用过去观察的记忆来增强状态表示。第17.3节简要讨论了一些可能的进一步扩展。


9.1 价值函数近似
-------------------

本书中涉及的所有预测方法都被描述为对估计价值函数的更新，该函数将其在特定状态下的值转换为该状态的“备份值”或 *更新目标*。
让我们使用符号 :math:`s \mapsto u` 的表示单独更新，其中 :math:`s` 是更新的状态，
:math:`u` 是 :math:`s` 的估计价值转移到的更新目标。
例如，价值预测的蒙特卡洛更新是 :math:`S_{t} \mapsto G_{t}`，
TD(0)更新是 :math:`S_{t} \mapsto R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbf{w}_{t})`，
n步TD更新为 :math:`S_{t} \mapsto G_{t:t+n}`。在DP（动态规划）中策略评估更新，
:math:`s\mapsto\mathbb{E}_{\pi}\left[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_{t})|S_{t}=s\right]`，
任意状态 :math:`s` 被更新，而在其他情况下，实际经验中遇到的状态 :math:`S_t` 被更新。

将每个更新解释为指定价值函数的所需输入-输出行为的示例是很自然的。
从某种意义上说，更新 :math:`s \mapsto u` 表示状态 :math:`s` 的估计值应更像更新目标 :math:`u`。
到目前为止，实际的更新是微不足道的：:math:`s` 的估计值的表条目已经简单地转移到了 :math:`u` 的一小部分，
并且所有其他状态的估计值保持不变。现在，我们允许任意复杂和精致方法来实现更新，并在 :math:`s` 处进行更新，以便更改许多其他状态的估计值。
学习以这种方式模拟输入输出示例的机器学习方法称为 *监督学习* 方法，当输出是像 :math:`u` 的数字时，该过程通常称为 *函数近似*。
函数近似方法期望接收它们试图近似的函数的期望输入-输出行为的示例。
我们使用这些方法进行价值预测，只需将每次更新的 :math:`s \mapsto u` 作为训练样例传递给它们。
然后，我们将它们产生的近似函数解释为估计价值函数。

以这种方式将每个更新视为传统的训练示例使我们能够使用任何广泛的现有函数近似方法来进行价值预测。
原则上，我们可以使用任何方法进行监督学习，例如人工神经网络，决策树和各种多元回归。
然而，并非所有函数近似方法都同样适用于强化学习。最复杂的人工神经网络和统计方法都假设一个静态训练集，在其上进行多次传递。
然而，在强化学习中，学习能够在线进行，而个体与其环境或其环境模型进行交互是很重要的。
要做到这一点，需要能够从增量获取的数据中有效学习的方法。
此外，强化学习通常需要函数近似方法能够处理非平稳目标函数（随时间变化的目标函数）。
例如，在基于GPI（广义策略迭代）的控制方法中，我们经常寻求在 :math:`\pi` 变化时学习 :math:`q_\pi`。
即使策略保持不变，如果训练示例的目标值是通过自举方法（DP和TD学习）生成的，则它们也是非平稳的。
不能轻易处理这种非平稳性的方法不太适合强化学习。


9.2 预测目标（:math:`\overline{\mathrm{VE}}`）
----------------------------------------------

到目前为止，我们尚未指定明确的预测目标。在表格的情况下，不需要连续测量预测质量，因为学习价值函数可以精确地等于真值函数。
此外，每个状态的学习价值都是分离的──一个状态的更新不受其他影响。
但是通过真正的近似，一个状态的更新会影响许多其他状态，并且不可能使所有状态的值完全正确。
假设我们有比权重更多的状态，所以使一个状态的估计更准确总是意味着让其他的不那么准确。
我们有义务说出我们最关心的状态。
我们必须指定状态分布 :math:`\mu(s)\geq 0,\sum_{s}\mu(s)=1`，表示我们关心每个状态 :math:`s` 中的错误的程度。
通过状态 :math:`s` 中的误差，我们指的是近似值 :math:`\hat{v}(s, \mathbf{w})` 与
真值 :math:`v_\pi(s)` 之间的差的平方。
通过 :math:`\mu` 对状态空间加权，我们得到一个自然目标函数，*均方误差*，表示为 :math:`\overline{\mathrm{VE}}`：

.. math::

    \overline{\mathrm{VE}}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s)\left[v_{\pi}(s)-\hat{v}(s, \mathbf{w})\right]^{2}
    \tag{9.1}

该度量的平方根（根 :math:`\overline{\mathrm{VE}}`）粗略地衡量了近似值与真实值的差异，并且通常用于图中。
通常 :math:`\mu(s)` 被选择为 :math:`s` 中花费的时间的一部分。
在在策略训练中，这被称为 *在策略分布*；我们在本章中完全关注这个案例。
在持续任务中，在策略分布是 math:`\pi` 下的固定分布。

.. admonition:: 回合任务中的在策略分布
    :class: note

    在一个回合任务中，在策略分布略有不同，因为它取决于如何选择回合的初始状态。
    设 ;math:`h(s)` 表示回合在每个状态 :math:`s` 中开始的概率，
    :math:`\eta(s)` 表示在一个回合中状态 :math:`s` 中平均花费的时间步数。
    如果回合以 :math:`s` 开头，或者如果从之前的状态 :math:`\overline{s}` 转换为 :math:`s`，则花费时间在状态 :math:`s` 中：

    .. math::

        \eta(s)=h(s)+\sum_{\overline{s}} \eta(\overline{s}) \sum_{a} \pi(a | \overline{s}) p(s | \overline{s}, a), \text { 对所有 } s \in \mathcal{S}
        \tag{9.2}

    可以针对预期的访问次数 :math:`\eta(s)` 求解该方程组。 然后，在策略分布是每个状态所花费的时间的一小部分，标准化和为一：

    .. math::

        \mu(s)=\frac{\eta(s)}{\sum_{s^{\prime}} \eta(s^{\prime})}, \quad \text { 对所有 } s \in \mathcal{S}
        \tag{9.3}

    这是没有折扣的自然选择。如果存在折扣（:math:`\gamma<1`），则应将其视为终止形式，
    这可以简单通过在（9.2）的第二项中包含因子 :math:`\gamma` 来完成。

这两种情况，即持续的和回合的，表现相似，但近似时必须在形式分析中单独处理，
正如我们将在本书的这一部分中反复看到的那样。这完成了学习目标的规范。

目前还不完全清楚 :math:`\overline{\mathrm{VE}}` 是加强学习的正确性能目标。
请记住，我们的最终目的──我们学习价值函数的原因──是找到更好的策略。
用于此目的的最佳价值函数不一定是最小化 :math:`\overline{\mathrm{VE}}` 的最佳值。
然而，目前尚不清楚价值预测的更有用的替代目标是什么。目前，我们将专注于 :math:`\overline{\mathrm{VE}}`。

就 :math:`\overline{\mathrm{VE}}` 而言，理想的目标是找到一个 *全局最优值*，
一个权重向量 :math:`\mathbf{w}^{*}`，对于所有可能的 :math:`\mathbf{w}`，
:math:`\overline{\mathrm{VE}}(\mathbf{w}^{*})\leq\overline{\mathrm{VE}}(\mathbf{w})`。
对于诸如线性函数近似器之类的简单函数逼近器，有时可以实现这一目标，
但对于诸如人工神经网络和决策树之类的复杂函数近似器来说很少是可能的。
除此之外，复杂函数近似器可以寻求收敛到 *局部最优*，一个权重向量 :math:`\mathbf{w}`，
对于 :math:`\mathbf{w}^{*}` 的某些邻域中的所有 :math:`\mathbf{w}` 满足
:math:`\overline{\mathrm{VE}}(\mathbf{w}^{*})\leq\overline{\mathrm{VE}}(\mathbf{w})`。
虽然这种保证只是稍微让人放心，但对于非线性函数近似器来说，它通常是最好的，而且通常它就足够了。
尽管如此，对于许多对强化学习感兴趣的情况，并不能保证收敛到最佳，甚至在最佳的有界距离内。
事实上，有些方法可能会出现发散，其 :math:`\overline{\mathrm{VE}}` 极限趋于无穷。

在前两节中，我们概述了一个框架，用于将价值预测的各种强化学习方法与各种函数近似方法相结合，使用前者的更新为后者生成训练样本。
我们还描述了这些方法可能希望最小化的 :math:`\overline{\mathrm{VE}}` 性能测量。
可能的函数近似方法的范围太大以至于不能覆盖所有方法，并且无论如何对其中的大多数方法进行可靠的评估或推荐知之甚少。
必要时，我们只考虑几种可能性。在本章的剩余部分，我们将重点放在基于梯度原理的函数近似方法，特别是线性梯度下降方法上。
我们关注这些方法的部分原因是因为我们认为这些方法特别有前途，因为它们揭示了关键的理论问题，同时也因为它们很简单，而且我们的空间有限。


9.3 随机梯度和半梯度方法
--------------------------


9.4 线性方法
----------------


9.5 线性方法的特征构造
------------------------

9.5.1 多项式
^^^^^^^^^^^^^^^

9.5.2 傅立叶基
^^^^^^^^^^^^^^^^^

9.5.3 粗编码（Coarse Coding）
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

9.5.4 平铺编码（Tile Coding）
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

9.5.5 径向基函数
^^^^^^^^^^^^^^^^^^^


9.6 手动选择步长参数
-----------------------


9.7 非线性函数近似：人工神经网络
---------------------------------


9.8 最小二乘TD
----------------------


9.9 基于内存的函数近似
-------------------------


9.10 基于核的函数近似
------------------------


9.11 深入研究在策略学习：兴趣和重点
------------------------------------


9.12 总结
--------------


书目和历史评论
---------------