第9章 在策略预测近似方法
===========================

在本章中，我们开始研究强化学习中的函数近似，考虑其在从在策略数据中估计状态-价值函数的用途，
即从使用已知策略 :math:`\pi` 生成的经验近似 :math:`v_\pi`。
本章的新颖之处在于，近似值函数不是用表格表示，
而表示成是具有权重向量 :math:`\mathbf{w} \in \mathbb{R}^{d}` 的参数化函数形式。
我们将权重向量 :math:`\mathbf{w}` 的状态 :math:`s` 的近似值写作 :math:`\hat{v}(s,\mathbf{w})\approx v_{\pi}(s)`。
例如，:math:`\hat{v}` 可能是状态特征中的线性函数，:math:`\mathbf{w}` 是特征权重的向量。
更一般地，:math:`\hat{v}` 可以是由多层人工神经网络计算的函数，其中 :math:`\mathbf{w}` 是所有层中的连接权重的向量。
通过调整权重，网络可以实现各种不同函数中的任何一种。
或者 :math:`\hat{v}` 可以是由决策树计算的函数，其中 :math:`\mathbf{w}` 是定义树的分裂点和叶值的所有数字。
通常，权重的数量（:math:`\mathbf{w}` 的维数）远小于状态的数量（:math:`d\ll|\mathcal{S}|`），
并且改变一个权重会改变许多状态的估计值。因此，当更新单个状态时，更改会从该状态推广到许多其他状态的值。
这种 *泛化* 使得学习可能更强大，但也可能更难以管理和理解。

也许令人惊讶的是，将强化学习扩展到函数近似也使其适用于部分可观察到的问题，其中个体无法获得完整状态。
如果 :math:`\hat{v}` 的参数化函数形式不允许估计值依赖于状态的某些方面，那么就好像这些方面是不可观察的。
实际上，本书这一部分中使用函数近似的方法的所有理论结果同样适用于部分可观察的情况。
然而，函数近似不能做的是用过去观察的记忆来增强状态表示。第17.3节简要讨论了一些可能的进一步扩展。
