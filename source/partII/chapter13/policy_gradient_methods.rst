第13章 策略梯度方法
====================

在本章中，我们考虑一些新内容。到目前为止，在本书中几乎所有的方法都是 *动作价值方法*。
他们了解了动作的价值，然后根据他们估计的行动价值选择行动 [1]_；如果没有动作价值估计，他们的策略甚至将不存在。
在本章中，我们考虑的方法是学习 *参数化策略*，该策略可以选择动作而无需咨询价值函数。
价值函数仍可用于 *学习* 策略参数，但动作选择不是必需的。
我们使用 :math:`\mathbf{\theta} \in \mathbb{R}^{d^{\prime}}` 表示策略的参数向量。
因此假设环境在时间 :math:`t` 时处于状态 :math:`s`，且参数为 :math:`\mathbf{\theta}`，
则在时间 :math:`t` 采取动作 :math:`a` 的概率为
:math:`\pi(a | s, \mathbf{\theta})={Pr}\{A_{t}=a | S_{t}=s, \mathbf{\theta}_{t}=\mathbf{\theta}\}`。
如果一种方法也使用学习的值函数，则该值函数的权重向量将通常表示为 :math:`\mathbf{w} \in \mathbb{R}^{d}`，
如 :math:`\hat{v}(s, \mathbf{w})` 所示。

在本章中，我们考虑基于一些基于标量性能指标 :math:`J(\mathbf{\theta})` 梯度的相对于策略参数的学习策略参数的方法。
这些方法试图使性能 *最大化*，因此它们的更新近似 :math:`J` 中的梯度 *上升*：

.. math::

    \mathbf{\theta}_{t+1}=\mathbf{\theta}_{t}+\alpha \widehat{\nabla J\left(\mathbf{\theta}_{t}\right)}
    \tag{13.1}

其中 :math:`\widehat{\nabla J\left(\mathbf{\theta}_{t}\right)} \in \mathbb{R}^{d^{\prime}}` 是一个随机估计，
其期望近似于性能度量相对于其参数 :math:`\mathbf{\theta}_{t}` 的梯度。
遵循此一般模式的所有方法，我们都称为 *策略梯度方法*，无论它们是否也学习近似值函数。
学习策略和值函数的近似值的方法通常称为 *演员-评论家* 方法，其中“演员”是指所学策略，而“评论家”是指所学习的值函数，通常是状态价值函数。
首先，我们处理回合案例，其中性能定义为参数化策略下的开始状态的值，
然后再继续考虑持续案例，其中性能定义为平均奖励率，如10.3节所述。
最后，我们能够以非常相似的形式表达两种情况的算法。


13.1 策略近似及其优势
----------------------


13.2 策略梯度定理
------------------


13.3 强化：蒙特卡罗策略梯度
---------------------------


13.4 带基线强化
------------------


13.5 演员-评论家方法
----------------------


13.6 持续问题的策略梯度
------------------------


13.7 持续动作的策略参数化
-------------------------


13.8 总结
------------


书目和历史评论
---------------


.. [1]
    唯一的例外是第2.8节中的梯度赌博机算法。实际上，在单状态赌博机的情况下，该部分将执行许多相同的步骤，就像我们在这里进行完整的MDP一样。
    复习该部分将为充分理解本章做好充分的准备。
