第1章 简介
===========

当我们思考学习的本质时，我们首先想到的是通过与环境互动来学习。
当一个婴儿玩耍，挥动手臂或环顾四周时，他没有明确的老师，但他确实通过直接的感觉与环境联系。
他可以通过这种联系获得大量关于因果关系、行动的结果以及如何实现目标的信息。
在我们的生活中，这种互动无疑是我们环境和自身知识的主要来源。
无论我们是学习驾驶汽车还是进行交谈，我们都敏锐地意识到我们的环境如何响应我们的行为，并且我们试图通过我们的行为来影响所发生的事情。
从互动中学习是几乎所有学习和智能理论的基本思想。

在本书中，我们探索了一种从交互中学习的 *计算* 方法。
我们主要探索理想化的学习情境并评估各种学习方法的有效性，而不是直接理解人或动物的学习方式 [#学习方式]_ 。也就是说，我们采用人工智能研究员或工程师的观点。
我们探索在解决科学或经济利益的学习问题方面有效的机器设计，通过数学分析或计算实验评估设计。
我们探索的方法称为 *强化学习*，更侧重于从交互中进行目标导向的学习，而不是其他机器学习方法。

.. [#学习方式] 第14章和第15章总结了心理学和神经科学的关系。

1.1 强化学习
------------

强化学习是一种学习如何将状态映射到动作，以获得最大奖励的学习机制。
学习者不会被告知要采取哪些行动，而是必须通过尝试来发现哪些行动会产生最大的回报。
在最有趣和最具挑战性的案例中，行动不仅可以影响直接奖励，还可以影响下一个状态，并通过下一个状态，影响到随后而来的奖励。
这两个特征 - 试错法和延迟奖励 - 是强化学习的两个最重要的可区分特征。

强化学习，就像许多名称以“ing”结尾的主题一样，例如机器学习和登山，同时也是一个问题，一类能够很好地解决问题的解决方法，以及研究这个问题及其解决方法的领域。
为所有三件事情使用单一名称是方便的，但同时必须保持三者（问题，方法，领域）在概念上的分离。
特别的，在强化学习中，区分问题和解决问题的方法是非常重要的；没有做出这种区分是许多混乱的根源。

我们使用动态规划理论的思想来规范化强化学习的问题，具体地说，是不完全已知的马尔可夫决策过程的最优控制。
这种规范化的详细描述将在第3章，但是最基本的思想是：采样实际问题最重要的方面，训练一个智能体多次与环境交互去达到一个目标。
智能体必须能够在某种程度上感知其环境状态，并且能够采取行动影响环境的状态。
智能体还必须具有与环境状态相关的一个或多个目标。
马尔科夫决策过程基本概念无差别的包含感知，行动和目标三个方面。
我们认为任何非常适合解决此类问题的方法都是一种强化学习方法。

强化学习不同于 *监督学习*。
监督学习是目前机器学习领域中研究最多的一种学习方式，它从知识渊博的教练所提供的有标记的训练集中学习。
每一个样例都是一种情况的描述，都带有标签，标签描述的是系统在该情况下的应该采取的正确动作，每一个样例用来区别这种情况应该属于哪一类。
这种学习的目的是让系统推断或概括它应有的反馈机制，使它可以对未知样本作出正确回应。
这是一种重要的学习方式，但单凭它并不足以从交互中学习。
在交互问题中，找到期待的既正确又典型的例子通常都是不切实际的。
在一个未知的领域，若要使收益最大化，智能体必须能够从自己的经验中学习。

强化学习也与机器学习研究人员所谓的 *无监督学习* 不同，后者通常是寻找隐藏在未标记数据集合中的结构。
监督学习和无监督学习这两个术语似乎对机器学习范式进行了详尽的分类，但事实却并非如此。
尽管人们可能会试图将强化学习视为一种无监督学习，因为它不依赖于正确行为的样例，强化学习试图最大化奖励信号而不是试图找到隐藏的结构。
在智能体的经验数据中揭示结构确实对强化学习特别有用，但是它本身并没有解决最大化奖励信号的强化学习问题。
因此，我们认为强化学习是第三种机器学习范式，除此之外还有监督学习和无监督学习，也许还有其他范式。

在强化学习中出现的其他类型的学习中未出现的挑战之一，是如何权衡探索与开发之间的关系。
为了获得大量奖励，强化学习智能体必须倾向于过去已经尝试过并且能够有效获益的行动。
但是要发现这样的行为，它必须尝试以前没有选择的行为。
智能体必须充分 *利用* 它既有经验以获得收益，但它也必须 *探索*，以便在未来做出更好的行动选择。
困境在于，任何探索和开发都难以避免失败。
智能体必须尝试各种行动，逐步地选择那些看起来最好的行动。
在随机任务中，每一个动作必须经过多次尝试才能得到可靠的预期收益。
几十年来，数学家一直在深入研究探索开发困境，但仍未得到解决。
就目前而言，在监督和无监督的学习中，至少在这些范式的最纯粹的形式中，完全平衡探索和开发的项目尚未出现。

强化学习的另一个关键特征是它明确地考虑了目标导向的智能体与不确定环境相互作用的 *整个* 问题。
这与许多考虑子问题但没有解决它们如何融入更大的图景的方法形成对比。
例如，我们已经提到很多机器学习研究都关注监督学习而没有明确说明这种能力最终如何有用。
其他研究人员已经制定了具有总体目标的规划理论，但没有考虑规划在实时决策中的作用，也没有考虑规划所需的预测模型来自何处的问题。
尽管这些方法已经产生了许多有用的结果，但它们一个重要的限制在于过于关注孤立子问题。

强化学习采取相反的策略，它具有一个完整的、交互式的、寻求目标的智能体。
所有强化学习智能体都有明确的目标，可以感知环境的各个方面，并可以选择影响其环境的行为。
此外，通常从一开始就假设智能体必须必须操作，尽管它面临的环境有很大的不确定性。
当强化学习涉及规划时，它必须解决规划和实时行动选择之间的相互作用，以及如何获取和改进环境模型的问题。
当强化学习涉及监督学习时，它要确定决定哪些能力是关键的，哪些是不重要的原因
为了学习研究以取得进步，必须隔离和研究重要的子问题，但它们应该是在完整的、交互式的、寻求目标的智能体中有明确功能的子问题，即使不能体现所有完整的细节。

一个完整的、交互式的、寻求目标的智能体，并不总是意味着像是一个完整的有机体或机器人
这里有许多明显的例子，但是一个完整的、交互式的、寻求目标的智能体也可以是更大行为系统的一个组成部分。
在这种情况下，智能体直接与较大系统的其余部分交互，并间接与较大系统的环境交互。
一个简单的例子是一个智能体，它监控机器人电池的充电水平并向机器人的控制架构发送命令。
这个智能体的环境是机器人的其余部分以及机器人的环境。
人们的眼光应超越最明显的智能体及其环境的例子，才能理解强化学习框架的普遍性。

现代强化学习最激动人心的一个方面是与其他工程和科学学科的实质性和富有成效的互动。
强化学习是人工智能和机器学习领域长达数十年的一个趋势，它与统计学、最优化和其他数学学科更紧密地结合在一起。
例如，某些强化学习方法学习参数的能力解决了运筹学与控制论中的经典“维数灾难”。
更有特色的是，强化学习也与心理学和神经科学有着紧密的联系，双方都有很大的好处。
在所有形式的机器学习中，强化学习最接近人类和其他动物所做的学习，而强化学习的许多核心算法最初都受到生物学习系统的启发。
通过动物学习的心理模型返回更符合经验数据的结果，以及通过一部分大脑奖励系统的有影响力的模型，强化学习也得到了反馈。
本书正文介绍了与工程和人工智能相关的强化学习的思想，并在第14章和第15章中总结了与心理学和神经科学的联系。

最后，强化学习也在某种程度上符合人工智能回归简单的一般性原则的一个大趋势。
自20世纪60年代后期以来，许多人工智能研究人员认为普遍性的原则是不存在的，而智能则归因于拥有大量特殊用途的技巧，过程和启发式方法。
有人说，如果我们能够将相关的事实充分地提供给一台机器，比如一百万或十亿，那么它就会变得聪明起来。
基于一般原则（如搜索或学习）的方法被定性为“弱方法”，而基于特定知识的方法被称为“强方法”。
这种观点在今天仍然很普遍，但并不占优势。
从我们的观点来看，这只是一个不成熟的过程：寻找一般原则的努力太少，以至于没有结论。
现代人工智能现在包括许多研究，寻找学习，搜索和决策的一般原则。
目前还不清楚钟摆会摆动多远，但强化学习研究肯定是摆向更简单和更少的人工智能一般原则的钟摆的一部分。

1.2 例子
--------

理解强化学习的一个好方法是思考其发展中的一些例子和可能的应用。

- 国际象棋大师采取行动。通过计划 - 预测可能的回复和反对 - 以及对特定位置和移动的可取性的直接，直观判断来通知选择。
- 自适应控制器实时调整炼油厂操作的参数。控制器在指定的边际成本的基础上优化产量/成本/质量交易，而不严格遵守工程师最初建议的设定点。
- 出生后几分钟，瞪羚小腿挣扎。半小时后，它以每小时20英里的速度运行。
- 移动机器人决定是否应该进入新房间以寻找更多垃圾来收集或开始尝试找回其电池充电站。它根据电池的当前充电水平以及过去能够快速轻松地找到充电器做出决定。
- 菲尔准备他的早餐。仔细检查，即使是这个看似平凡的活动，也会发现一个复杂的条件行为网和互锁的目标，
  子目标关系：走到橱柜，打开它，选择一个谷物盒，然后伸手去拿，抓住并取回盒子。
  需要其他复杂的，调整的，交互式的行为序列来获得碗，勺子和牛奶盒。每个步骤都涉及一系列眼球运动，以获取信息并指导到达和运动。
  对于如何携带物品或者在获得其他物品之前将它们中的一些运送到餐桌上是否更好地做出快速判断。
  每个步骤都以目标为指导，例如抓勺子或到冰箱，并且服务于其他目标，例如一旦谷物准备好就吃勺子并最终获得营养。
  无论他是否意识到这一点，菲尔都在获取有关他身体状况的信息，这些信息决定了他的营养需求，饥饿程度和食物偏好。

这些示例共享的功能非常基本，很容易被忽略。
所有这些都涉及积极的决策智能体与其环境之间的 *互动*，智能体在存在不确定性的环境中寻求实现 *目标*。
智能体的行为能影响未来的环境状态（例如，下一个国际象棋位置，炼油厂的水库水位，机器人的下一个位置以及其电池的未来充电水平），从而影响智能体之后可以采取的行动和机会。
正确的选择需要考虑到行动的间接延迟后果，因此可能需要预见或规划。

同时，在所有这些例子中，行动的效果都无法完全预测; 因此，智能体必须经常监控其环境并做出适当的反应。
例如，菲尔必须观察他倒入谷物碗中的牛奶以防止溢出。
所有这些例子都涉及明确的目标，即智能体可以根据其直接感知的内容判断实现目标的进度。
国际象棋选手知道他是否获胜，炼油厂控制员知道生产了多少石油，瞪羚小牛知道它何时落下，移动机器人知道它的电池何时耗尽，菲尔知道他是否正在享用他的早餐。

在所有这些示例中，智能体可以使用其经验来改善其性能。
国际象棋选手改进了他用来评估位置的直觉，从而改善了他的发挥; 瞪羚小牛提高了它的活力; 菲尔学会精简他的早餐。
智能体在任务开始时所具有的的知识 - 无论是之前的相关任务经验还是通过设计或演变带来的 - 都会影响有用或易于学习的内容，
但与环境的交互对于调整行为以利用任务的特性更加至关重要。

1.3 强化学习的要素
------------------

1.4 Limitations and Scope
--------------------------

1.5 An Extended Example: Tic-Tac-Toe
--------------------------------------

1.6 Summary
-----------

1.7 Early History of Reinforcement Learning
--------------------------------------------
