第15章 神经科学
====================

神经科学是对神经系统的多学科研究的总称，主要包括：如何调节身体功能，如何控制行为，
由发育、学习和老化所引起的随着时间的变化，以及细胞和分子机制如何使这些功能成为可能。
强化学习的最令人兴奋的方面之一是来自神经科学的越来越多的证据表明，
人类和许多其他动物的神经系统实施的算法和强化学习算法在很多方面是一一对应的。
本章主要解释这些相似之处，以及他们对动物的基于收益的学习的神经基础的看法。

强化学习和神经科学之间最显著的联系就是多巴胺，它是一种哺乳动物大脑中与收益处理机制紧密相关的化学物质。
多巴胺的作用就是将TD误差传达给进行学习和决策的大脑结构。
这种相似的关系被表示为 *多巴胺神经元活动的收益预测误差假说*，这是由强化学习和神经科学实验结果引出的一个假设。
在本章中我们将讨论这个假设，引出这个假设的神经科学发现，以及为什么它对理解大脑收益系统有重要作用。
我们还会讨论强化学习和神经科学之间的相似之处，虽然这种相似不如多巴胺／TD误差之间的相似那么明显，
但它提供了有用的概念工具，用于研究动物的基于收益的学习机制。
强化学习的其他元素也有可能会影响神经系统的研究，但是本章对它们与神经科学之间的联系相对讨论得不多。
我们在本章只讨论一些我们认为随着时间的推移会变得重要的联系。

正如我们在本书第1章的强化学习的早期历史部分（1.7节）所概述的，强化学习的许多方面都受到神经科学的影响。
本章的第二个目标是向读者介绍有关脑功能的观点，这些观点对强化学习方法有所贡献。
从脑功能的理论来看，强化学习的一些元素更容易理解。
对于“资格迹”这一概念尤其如此，资格迹是强化学习的基本机制之一起源于突触的一个猜想性质（突触是神经细胞与神经元之间相互沟通的结构）。

在本章，我们并没有深入研究动物的基于收益学习的复杂神经系统，因为我们不是神经科学家。
我们并不试图描述（甚至没有提及）许多大脑结构，或任何分子机制，即使它们都被认为参与了这些过程。
我们也不会对与强化学习非常吻合的假设和模型做出评判。神经科学领域的专家之间有不同的看法是很正常的。
我们仅仅想给读者讲好有吸引力和建设性的例子。
我们希望这一章给读者展现多种将强化学习及其理论基础与动物的基于收益学习的神经科学理论联系起来的渠道。

许多优秀的著作介绍了强化学习与神经科学之间的联系，我们在本章的最后一节中引用了其中的一些。
我们的方法和这些方法不太相似因为我们假设读者熟悉本书前面几章所介绍的强化学习，但是不了解有关神经科学的知识。
因此我们首先简要介绍神经科学的概念，以便让你有基本的理解。


15.1 神经科学基础
------------------

了解一些关于神经系统的基本知识有助于理解本章的内容。我们后面提到的术语用楷体表示。
如果你已经有神经科学方面的基本知识，则可以跳过这一节。

*神经元* 是神经系统的主要组成部分，是专门用于电子和化学信号的处理及信息传输的细胞。
它们以多种形式出现，但神经元通常具有细胞体、*树突* 和单个 *轴突*。
树突是从细胞体分叉出来，以接收来自其他神经元的输入（或者在感觉神经元的情况下还接收外部信号）的结构。
神经元的轴突是将神经元的输出传递给其他神经元（或肌肉、腺体）的纤维。
神经元的输出由被称为 *动作电位* 的电脉冲序列构成，这些电脉冲沿着轴突传播。
动作电位也被称为 *尖峰*，而神经元在产生尖峰时被认为是触发的。
在神经网络模型中，通常使用实数来表示神经元的 *放电速率*，即每单位时间的平均放电次数。

神经元的轴突可以分很多叉，使神经元的动作电位达到许多目标。神经元轴突的分叉结构部分被称为神经元的 *轴突中枢*。
因为动作电位的传导是一个主动过程，与导火索的燃烧不同，所以当动作电位到达轴突的分叉点时，
它会“点亮”所有输出分支上的动作电位（尽管有时会无法传播到某个分支）。
因此，具有大型轴突中枢的神经元的活动可以影响许多目标位置。

*突触* 通常是轴突分叉终止处的结构，作为中介调整一个神经元与另一个神经元之间的通信。
突触将信息从 *突触前* 神经元的轴突传递到 *突触后* 神经元的树突或细胞体。
除少数例外，当动作电位从突触前神经元传输到突触的时候突触会释放化学 *神经递质*
（但有时神经元之间有直接电耦合的情况，但是在这里我们不涉及这些）从突触的前侧释放的神经递质分子会弥漫在 *突触间隙*，
即突触前侧的末端和突触后神经元之间的非常小的空间，
然后与突触后神经元表面的受体结合，以激发或抑制其产生尖峰的活性，或以其他方式调节其行为。
一种特定的神经递质可能与几种不同类型的受体结合，每种受体在突触后神经元上产生不同的反应。
例如，神经递质多巴胺至少可以通过五种不同类型的受体来影响突触后神经元。
许多不同的化学物质已被确定为动物神经系统中的神经递质。

神经元的 *背景* 活动指的是“背景”情况下的活动水平，通常是它的放电速率。
所谓“背景情况”是指神经元的活动不是由实验者指定的任务相关的突触输入所驱动的，
例如，当神经元的活动与作为实验的一部分传递给被试者的刺激无关时，我们就认为其活动是背景活动。
背景活动可能由于输入来自于更广泛的网络而具有不规则性，或者由于神经或突触内的噪声而显得不规则。
有时背景活动是神经元固有的动态过程的结果。与其背景活动相反，神经元的 *阶段性* 活动通常由突触输入引起的尖峰活动冲击组成。
对于那些变化缓慢、经常以分级的方式进行的活动，无论是否是背景活动，都被称为神经元的 *增补* 活动。

突触释放的神经递质对突触后神经元产生影响的强度或有效性就是突触的 *效能*。
一种利用经验改变神经系统的方式就是通过改变突触的效能来改变神经系统，
这个“效能”是突触前和突触后神经元的活动的组合产生的结果，有时也来自于神经调节剂产生的结果。
所谓 *神经调节剂*，就是除了实现直接的快速兴奋或抑制之外，还会产生其他影响的神经递质。

大脑含有几个不同的神经调节系统，由具有广泛分叉的树状轴突神经元集群组成，每个系统使用不同的神经递质。
神经调节可以改变神经回路的功能、中介调整的动因、唤醒、注意力、记忆、心境、情绪、睡眠和体温。
这里重要的是，神经调节系统可以分配诸如强化信号之类的标量信号以改变突触的操作，
这些突触往往广泛分布在不同地方但对神经元的学习具有关键作用。

突触效能变化的能力被称为 *突触的可塑性*。
这是学习活动的主要机制之一通过学习算法调整的参数或权重对应于突触的效能（synaptic efficacies）正如我们下面要详细描述的，
通过神经调节剂多巴胺对突触可塑性进行调节是大脑实现学习算法的一种机制，就像本书所描述的那些算法一样。


15.2 收益信号、强化信号、价值和预测误差
----------------------------------------

神经科学和计算型的强化学习之间的联系始于大脑信号和在强化学习理论与算法中起重要作用的信号之间的相似性。
在第3章中，我们提到，任何对目标导向的行为进行学习的问题描述都可以归结为具有代表性的三种信号：动作、状态和收益。
然而，为了解释神经科学和强化学习之间的联系，我们必须更加具体地考虑其他强化学习的信号，这些信号以特定的方式与大脑中的信号相对应。
除了收益信号以外，还包含强化学习信号（我们认为这些信号不同于收益信号）、价值信号和传递预测误差的信号。
当我们以某种方式用对应函数来标记一个信号的时候，我们就在强化学习理论的语境之下把信号和某个公式或算法中的一项对应起来。
另一方面，当我们提到大脑中的一个信号时，也是想表示一个生理事件，比如动作电位的突变或者神经递质的分泌。
把一个神经信号标记为对应函数，比如把一个多巴胺神经元相位活动称为一个强化信号，意味着我们推测这个神经信号的作用与强化学习理论中的信号作用类似。

找到这些对应关系的证据面临诸多挑战。与收益处理过程相关的神经活动几乎可以在大脑的每一个部分找到，
但是由于不同的信号通常具有高度相关性，因此我们很难清楚地解释结果。
我们需要设计严谨的实验来把一种类型的收益相关信号和其他类型的收益信号区别开来，或者和其他与收益过程无关的大量信号区别开来。
尽管存在这些困难，但我们已经进行了许多实验来使强化学习理论和算法与神经信号对应起来，并建立一些具有说服力的联系。
为了在后续章节中说明这些联系，在本节的后面我们将告诉读者各种收益相关的信号与强化学习理论中信号的对应关系。

在第14章末介绍术语时，我们说到的 :math:`R_{t}` 更像动物大脑中的收益信号，而非动物环境中的物体或事件。
收益信号（以及智能体的环境）定义了强化学习智能体正试图解决的问题。
就这一点而言，:math:`R_{t}` 就像动物大脑中的一个信号，定义收益在大脑各个位置的初始分布。
但是在动物的大脑中不可能存在像 :math:`R_{t}` 这样的统一的收益信号。
我们最好把 :math:`R_{t}` 看作一个概括了大脑中许多评估感知和状态奖惩性质的系统产生的大量神经信号整体效应的抽象。

强化学习中的 *强化信号* 与收益信号不同。强化信号的作用是在一个智能体的策略、价值估计或环境模型中引导学习算法做出改变。
对于时序差分方法，例如，:math:`t` 时刻的强化信号是TD误差 :math:`\delta_{t-1}=R_{t}+\gamma V(S_{t})-V(S_{t-1})` [1]_。
某些算法的强化信号可能仅仅是收益信号，但是大多数是通过其他信息调整过的收益信号，例如TD误差中的价值估计。

状态价值函数或动作价值函数的估计，即 :math:`V` 或 :math:`Q`，指明了在长期内对智能体来说什么是好的，什么是坏的。
它们是对智能体未来期望积累的总收益的预测。智能体做出好的决策，
就意味着选择合适的动作以到达具有最大估计状态价值的状态，或者直接选择具有最大估计动作价值的动作。

.. [1]
    如我们在6.1节中介绍的，在我们的符号体系下 :math:`\delta_{t}` 被定义为 :math:`R_{t+1}+\gamma V(S_{t+1})-V(S_{t})`，
    所以，只有到了 :math:`t+1` 时刻才能得到 :math:`\delta_{t}`。
    则 :math:`t` 时刻的TD误差实际是 :math:`\delta_{t-1}=R_{t}+\gamma V\left(S_{t}\right)-V\left(S_{t-1}\right)`。
    因为我们通常认为每个时间步长是非常小甚至有时可以认为是无限小的，以对于定义上面这样的单个时刻的偏移不需要过分解读它的重要性。

预测误差衡量期望和实际信号或感知之间的差异。收益预测误差（reward prediction errors，RPE）衡量期望和实际收到的收益信号之的差异，
当收益信号大于期望时为正值，否则为负值。像式（6.5）中的TD误差是特殊类的RE，它表示当前和早先的长期回报期望之间的差异。
当神经科学家提到RPE时，他们一般（但不总是）指 TD RPE，在本章中我们简单地称之为TD误差。
在本章中TD误差通常不依赖于动作，不同于在 Sarsa和Q－学习算法中学习动作价值时的TD误差。
这是因为最明显的与神经科学的联系是用动作无关的TD误差来表述的，但是这并不意味着不存在与动作相关的TD误差的联系
（用于预测收益以外信号的TD误差也是有用的，但我们不加以考虑，这类例子可以参考 Modayil、 White和 Sutton，2014）。

关于神经科学数据与这些从理论上定义的信号之间的联系，我们可以提很多问题。
比如，观测到的信号更像一个收益信号、价值信号预测误差、强化信号，还是一个完全不同的东西？
如果是误差信号，那是收益预测误差（RE）、TD误差，还是像 Rescorla－Wagner误差（式14.3）这样的更简单的误差？
如果是TD误差，那是否是动作相关的“Q学习”或 Sarsa 等误差？如上所述，通过探索大脑来回答这样的问题是非常困难的。
但实验证据表明，一种神经递质，特别是多巴胺，表示RPE信号，而且生产多巴胺的神经元的相位活动事实上会传递TD误差（见15.1节节关于相位活动的定义）。
这个证据引出了 *多巴胺神经元活动的收益预测误差假说*，我们将在下面描述。


15.3 收益预测误差假说
-----------------------

*多巴胺神经元活动的收益预测误差假说* 认为，哺乳动物体内产生多巴胺的神经元的相位活动的功能之一，
就是将未来的期望收益的新旧估计值之间的误差传递到整个大脑的所有目标区域。
Montague、 Dayan和 Sejnowski1996）首次明确提出了这个假说（虽然没有用这些确切的词语），
他们展示了强化学习中的TD误差概念是如何解释哺乳动物中多巴胺神经元相位活动各种特征的。
引出这一假说的实验于20世纪80年代、90年代初在神经科学家沃尔夫拉姆·舒尔茨的实验室进行。
15.4节描述了这些重要实验，15.6节解释了这些实验的结果与TD误差的一致性，
本章末尾的参考文献和历史评注部分包含了记录这个重要假设发展历程的文献。

Montague等人（1996）比较了经典条件反射下时序差分模型产生的TD误差和经典条件反射环境下产生多巴胺的神经元的相位活动。
回顾14.2节，经典条件反射下的时序差分模型基本上是线性函数逼近的半梯度下降TD（:math:`\lambda`）算法。
Montague等人做了几个假设来进行对比。首先，由于TD误差可能是负值，但神经元不能有负的放电速率，
所以他们假设与多巴胺神经元活动相对应的量是 :math:`\delta_{t-1}+b_{t}`，其中 :math:`b_t` 是神经元的背景放电速率。
负的TD误差对应于多巴胺神经元低于其背景放电速率的放电速率降低量 [2]_。

第二个假说是关于每次经典条件反射试验所访问到的状态以及它们作为学习算法的输入量的表示方式的。
我们在14.2.4中针对时序差分模型讨论过这个问题。
Montague等人选择了全串行复合刺激表示（CSC），如图141左边一列所示，但略有不同的是，
短期内部信号的序列一直持续到US开始出现，而这里就是非零收益信号到达的地方。
这种表示方式使得TD误差能够模仿这样一种现象：多巴胺神经元活动不仅能预测未来收益，也对收到预测线索之后，收益 *何时* 可以达成是敏感的。
我们必须有一些方法来追踪感官线索和收益达成之间的间隔时间。
如果一个刺激对其后会继续产生的内部信号的序列进行了初始化，并且它们在刺激结束之后的每个时刻都产生不同的信号，
那么在每个时刻，我们可以用不同的状态来表示这些信号。因此，依赖于状态的TD误差对试验中事件发生的时间是敏感的。

有了这些关于背景放电速率和输入表示的假说，在15.5节的模拟试验中，时序差分模型的TD误差与多巴胺神经元的相位活动就十分相似了。
在15.5节中我们对这些相似性细节进行了描述，TD误差与多巴胺神经元的下列特征是相似的：
1）多巴胺神经元的相位反应只发生在收益事件不可预测时；
2）在学习初期，在收益之前的中性线索不会引起显著的相位多巴胺反应，但是随着持续的学习，这些线索获得了预测值并随即引起了相位多巴胺反应；
3）如果存在比已经获得预测值的线索更早的可靠线索，则相位多巴胺反应将会转移到更早的线索，并停止寻找后面的线索；
4）如果经过学习之后，预测的收益事件被遗漏，则多巴胺神经元的反应在收益事件的期望时间之后不久就会降低到其基准水平之下。

虽然在 Schultz等人的实验中，并不是每一个被监测到的多巴胺神经元都有以上这些行为，
但是大多数被监测神经元的活动和TD误差之间惊人的对应关系为收益预测误差假说提供了强有力的支持。
然而，仍存在一些情况基于假设的预测与实验中观察到的不一致。
输入表示的选择对于TD误差与多巴胺神经元活动某些细节之间的匹配程度来说至关重要，特别是多巴胺神经元的反应时间的细节。
为了使二者更加吻合，有一些关于输入表示和时序差分学习其他特征的不同思想被提了出来，
我们会在下面讨论一些，但主流的表示方法还是 Montague等人的CSC表示方法。
总体而言，收益预测误差假说已经在研究收益学习的神经科学家中被广泛接受，并且已经被证明能适应来自神经科学实验的更多结果。

为描述支持收益预测误差假说的神经科学实验，我们会提供一些背景使得假设的重要性更容易被理解。
我们接下来介绍一些关于多巴胺的知识，和它影响的大脑结构，以及它们是如何参与收益学习过程的。

.. [2]
    多巴胺神经元活动相关的TD误差中的 :math:`\delta_t` 与
    我们的 :math:`\delta_{t-1}=R_{t}+\gamma V(S_{t})-V(S_{t-1})` 是类似的。

