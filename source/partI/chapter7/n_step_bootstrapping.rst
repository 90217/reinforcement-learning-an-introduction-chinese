第7章 多步引导（Bootstrapping）方法
======================================

在本章中，我们统一了蒙特卡罗（MC）方法和前两章中介绍的一步时序差分（TD）方法。MC方法和一步法TD方法都不是最好的。
在本章中，我们将介绍 *n步TD方法*，这些方法概括了两种方法，以便可以根据需要平滑地从一种方法转换到另一种方法，以满足特定任务的需求。
n步方法在一端采用MC方法，在另一端采用一步TD方法。最好的方法通常介于两个极端之间。

另一种看待n步方法的好处的方法是让它们摆脱时间步骤的独裁。
使用一步TD方法，同一时间步骤确定可以更改操作的频率以及完成引导的时间间隔。
在许多应用程序中，人们希望能够非常快速地更新动作以考虑已经发生变化的任何事情，
但是如果引导在一段时间内发生了重大且可识别的状态变化，则引导效果最佳。
使用一步TD方法，这些时间间隔是相同的，因此必须做出妥协。
n步方法使引导能够在多个步骤中发生，使我们摆脱单一时间步骤的暴政。

n步方法的概念通常用作 *资格跟踪* （第12章）算法思想的介绍，它可以同时实现多个时间间隔的引导。
在这里，我们反过来考虑n步引导的想法，将资格跟踪机制的处理推迟到以后。
这使我们能够更好地分离问题，在更简单的n步设置中处理尽可能多的问题。

像往常一样，我们首先考虑预测问题，然后考虑控制问题。
也就是说，我们首先考虑n步方法如何帮助预测作为固定策略的状态函数的回报（即，估计 :math:`v_\pi`）。
然后我们将想法扩展到行动价值和控制方法。


7.1 多步TD预测
---------------

蒙特卡罗和TD方法之间的方法空间是什么？考虑使用 :math:`\pi` 生成的样本回合估计v⇡。
蒙特卡罗方法基于从该状态到回合结束的观察到的奖励的整个序列来执行每个状态的更新。
另一方面，一步法TD方法的更新仅基于下一个奖励，一步之后从状态价值引导作为剩余奖励的代理。
然后，一种中间方法将基于中间数量的奖励执行更新：多于一个，但是在终止之前少于所有奖励。
例如，两步更新将基于前两个奖励以及两个步骤后的估计的状态值。
同样，我们可以进行三步更新，四步更新等。图7.1显示了 :math:`v_\pi` 的 *n步更新* 频谱的备份图，
左侧是一步TD更新，右侧是直到最后终止的蒙特卡罗更新。

.. figure:: images/figure-7.1.png

    **图7.1：** n步方法的备份图。这些方法组成从一步TD方法到蒙特卡罗方法的频谱范围。

使用n步更新的方法仍然是TD方法，因为它们仍然根据它从后来的估计中如何变化来改变先前的估计。
现在后来的估计不是一步之后，而是n步之后。时序差分在n个步骤上延伸的方法称为 *n步TD方法*。
前一章介绍的TD方法都使用了一步更新，这就是我们称之为一步TD方法的原因。

更正式地，考虑状态 :math:`S_t` 的估计值的更新，作为状态奖励序列的结果，
:math:`S_{t}, R_{t+1}, S_{t+1}, R_{t+2}, \ldots, R_{T}, S_{T}` （省略动作）。
我们知道在蒙特卡洛更新中，:math:`v_\pi(S_t)` 的估计值会在完全回报的方向上更新：

.. math::

    G_{t} \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots+\gamma^{T-t-1} R_{T}

其中T是回合的最后一个步骤。我们将此数量称为更新的 *目标*。
而在蒙特卡洛更新中目标是回报，在一步更新中，目标是第一个奖励加上下一个状态的折扣估计值，我们称之为 *一步回报*：

.. math::

    G_{t : t+1} \doteq R_{t+1}+\gamma V_{t}\left(S_{t+1}\right)

其中这里的 :math:`V_{t} : \mathcal{S} \rightarrow \mathbb{R}` 是 :math:`v_\pi` 在时刻t的估计值。
:math:`G_{t:t+1}` 的下标表示它是时间t的截断回报，使用奖励直到时间 :math:`t+1`，
折扣估计 :math:`\gamma V_{t}\left(S_{t+1}\right)` 代替其他项
:math:`\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots+\gamma^{T-t-1} R_{T}` 的完全回报，
如前一章所述。我们现在的观点是，这个想法在经过两个步骤之后就像在一个步骤之后一样有意义。
两步更新的目标是两步回报：

.. math::

    G_{t : t+2} \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} V_{t+1}\left(S_{t+2}\right)

其中现在 :math:`\gamma^{2} V_{t+1}\left(S_{t+2}\right)` 纠正了项
:math:`\gamma^{2} R_{t+3}+\gamma^{3} R_{t+4}+\cdots+\gamma^{T-t-1} R_{T}` 的缺失。
同样，任意n步更新的目标是n步回报：

.. math::

    G_{t : t+n} \doteq R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1} R_{t+n}+\gamma^{n} V_{t+n-1}\left(S_{t+n}\right)
    \tag{7.1}

而所有其他状态的值保持不变：对于所有 :math:`s \neq S_{t}`，:math:`V_{t+n}(s)=V_{t+n-1}(s)`。
我们将此算法称为 *n步TD*。 请注意，在每回合的前n-1个步骤中，根本不会进行任何更改。
为了弥补这一点，在回合结束后，终止后和开始下一集之前，会进行相同数量的额外更新。
完整的伪代码在下面的框中给出。

.. admonition:: 多步TD(0)估计 :math:`V \approx v_\pi`
    :class: important

    输入：策略 :math:`\pi`

    算法参数：步长 :math:`\alpha \in (0,1]`，正整数 :math:`n`

    对 :math:`s \in \mathcal{S}`，任意初始化 :math:`V(s)`

    所有存储和访问操作（对于 :math:`S_t` 和 :math:`R_t`）都可以使其索引 :math:`mod n + 1`

    对每个回合循环：

        初始化并存储 :math:`S_0 \ne` 终点

        :math:`T \leftarrow \infty`

        对 :math:`t=0,1,2, \ldots` 循环：

            如果 :math:`t < T`，则：

                根据 :math:`\pi(\cdot|S_t)` 采取行动

                观察并将下一个奖励存储为 :math:`R_{t+1}`，将下一个状态存储为 :math:`S_{t+1}`

                如果 :math:`S_{t+1}` 是终点，则 :math:`T \leftarrow t+1`

            :math:`\tau \leftarrow t - n + 1` （:math:`\tau` 是状态估计正在更新的时间）

            如果 :math:`\tau > 0`：

                :math:`G \leftarrow \sum_{i=\tau+1}^{\min (\tau+n, T)} \gamma^{i-\tau-1} R_{i}`

                如果 :math:`\tau + n < T`， 则 :math:`G \leftarrow G+\gamma^{n} V\left(S_{\tau+n}\right)`

                :math:`V\left(S_{\tau}\right) \leftarrow V\left(S_{\tau}\right)+\alpha\left[G-V\left(S_{\tau}\right)\right]`  :math:`\quad\quad\quad`   :math:`\left(G_{\tau : \tau+n}\right)`

        直到 :math:`\tau = T - 1`

*练习7.1* 在第6章中，我们注意到如果值估计值不是逐步变化，则蒙特卡罗误差可写为TD误差之和（6.6）。
证明（7.2）中使用的n步误差也可以写为推导早期结果的总和TD误差（如果值估计没有改变则再次）。

*练习7.2（编程）* 使用n步法，值估计 *会* 逐步变化，因此使用TD误差之和（参见上一个练习）代替（7.2）中的误差的算法实际上的算法略有不同。
它会是一个更好的算法还是更糟糕的算法？设计和编写一个小实验来根据经验回答这个问题。


7.2 多步Sarsa
---------------


7.3 多步离策略学习
------------------


7.4 \*具有控制变量的各决策方法
------------------------------


7.5 无重要性采样的离策略学习：n步树备份算法
---------------------------------------------


7.6 \*统一算法：n步 :math:`Q(\sigma)`
--------------------------------------


7.7 总结
----------


书目和历史评论
---------------
