第8章 表格方法规划和学习
========================

在本章中，我们开发了强化学习方法的统一视图，这些方法需要环境模型，例如动态规划和启发式搜索，
以及可以在没有模型的情况下使用的方法，例如蒙特卡罗和时序差分方法。
这些分别称为 *基于模型（model-based）* 和 *不基于模型（model-free）*的强化学习方法。
基于模型的方法依赖于 *规划（planning）* 作为其主要组成部分，而不基于模型方法主要依赖于 *学习（learning）*。
虽然这两种方法之间存在着真正的差异，但也存在很大的相似之处。
特别是，这两种方法的核心是价值函数的计算。此外，所有方法都基于展望未来事件，计算备份值，然后将其用作近似值函数的更新目标。
在本书的前面，我们将蒙特卡洛和时序差分方法作为不同的替代方法，然后展示了如何通过n步方法统一它们。
我们在本章中的目标是基于模型和不基于模型方法的类似集成。
在前面的章节中已经确定了这些不同之处，我们现在探讨它们可以混合的程度。


8.1 模型和规划
-----------------

通过环境 *模型*，我们指的是个体可以用来预测环境如何响应其行为的任何事物。
给定状态和动作，模型产生对结果下一状态和下一奖励的预测。
如果模型是随机的，那么有几种可能的下一个状态和下一个奖励，每个都有一定的发生概率。
一些模型描述了所有可能性及其概率；这些我们称之为 *分布模型*。
其他模型只产生一种可能性，根据概率进行采样；这些我们称之为 *采样模型*。
例如，考虑对十几个骰子的总和进行建模。
分布模型将产生所有可能的总和及其发生的概率，而样本模型将产生根据该概率分布绘制的单个总和。
在动态规划中假设的模型──MDP动力学的估计 :math:`p\left(s^{\prime}, r | s, a\right)` ──是分布模型。
第5章中二十一点示例中使用的模型是一个采样模型。分布模型比样本模型更强大，因为它们可以始终用于产生样本。
但是，在许多应用中，获取样本模型比分布模型容易得多。十几个骰子就是一个简单的例子。
编写一个计算机程序来模拟骰子滚动并返回总和是很容易的，但是找出所有可能的总和及其概率更难且更容易出错。

模型可用于模仿或模拟体验。给定起始状态和动作，样本模型产生可能的转换，分布模型生成由其发生概率加权的所有可能转换。
给定起始状态和策略，样本模型可以产生整个回合，分布模型可以生成所有可能的回合及其概率。
在这两种情况下，我们都说模型用于 *模拟* 环境并产生 *模拟经验*。

在不同领域中，*规划* 这个词以几种不同的方式使用。
我们使用该术语来指代将模型作为输入并生成或改进与建模环境交互的策略的任何计算过程：

.. math::

    \text{模型} \stackrel{规划}{\longrightarrow} \text{策略}

在人工智能中，根据我们的定义，有两种不同的规划方法。*状态空间规划*，包括我们在本书中采用的方法，
主要被视为通过状态空间搜索最优策略或目标的最佳路径。动作导致从状态到状态的转换，并且价值函数在状态上计算。
在我们所说的 *规划空间规划* 中，规划是通过规划空间进行搜索。
操作将一个规划转换为另一个规划，并在规划空间中定义价值函数（如果有）。
规划空间规划包括演化方法和“偏序规划”，这是人工智能中的一种常见规划，其中步骤的排序在规划的所有阶段都没有完全确定。
规划空间方法难以有效地应用于强化学习中关注的随机序列决策问题，我们不会进一步考虑它们（但参见，例如，Russell和Norvig，2010）。

我们在本章中提出的统一观点是，所有的状态空间规划方法都有一个共同的结构，这种结构也存在于本书所介绍的学习方法中。
本章的其余部分将开发此视图，但有两个基本思想：
（1）所有状态空间规划方法都将计算价值函数作为改进策略的关键中间步骤，
（2）它们通过应用于模拟经验的更新或备份操作计算价值函数。
这种通用结构可以表示如下：

.. math::

    \text{模型} \longrightarrow \text{模拟经验} \stackrel{备份}{longrightarrow} \text{价值} \longrightarrow \text{策略}

动态规划方法显然适合这种结构：它们扫描状态空间，为每个状态生成可能的转换分布。
然后，每个分布用于计算备份值（更新目标）并更新状态的估计值。
在本章中，我们认为各种其他状态空间规划方法也适合这种结构，
各种方法的区别仅在于它们的更新类型，它们的执行顺序以及备份信息的保留时间。

以这种方式查看规划方法强调了它们与我们在本书中描述的学习方法的关系。
学习和规划方法的核心是通过备份更新操作来估计价值函数。
不同之处在于，规划使用模型生成的模拟经验，而学习方法则使用环境产生的实际经验。
当然，这种差异导致了许多其他差异，例如，如何评估性能以及如何灵活地产生经验。
但是，共同的结构意味着许多想法和算法可以在规划和学习之间转移。
特别地，在许多情况下，学习算法可以代替规划方法的关键更新步骤。
学习方法仅需要经验作为输入，并且在许多情况下，它们可以应用于模拟经验以及真实经验。
下面的框显示了基于一步表格Q-learning的规划方法和来自样本模型的随机样本的简单示例。
这种方法，我们称之为 *随机样本一步表格Q-planning*，
在一步表格Q-learning收敛于真实环境的最优政策的相同条件下，收敛到模型的最优策略
（必须在步骤1中无限次地选择每个状态-动作，并且 :math:`\alpha` 必须随时间适当减小）。

.. admonition:: 随机样本一步表格Q-planning
    :class: important

    一直循环：

        1. 随机选择一个状态 :math:`S\in\mathcal{S}` 和动作 :math:`A\in\mathcal{A}`

        2. 将 :math:`S, A` 发送到样本模型，并获得样本下一个奖励 :math:`R` 和样本下一个状态 :math:`S^{\prime}`

        3. 对 :math:`S,A,R,S^{\prime}` 应用一步表格Q-learning：

            :math:`Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right]`

除了统一的规划和学习方法之外，本章的第二个主题是以小的渐进步骤进行规划的好处。
这使规划能够在几乎没有浪费的计算的情况下随时中断或重定向，这似乎是将计划与行为和学习模型有效混合的关键要求。
如果问题太大而无法准确解决，即使在纯粹的规划问题上，以非常小的步骤进行规划也可能是最有效的方法。


8.2 Dyna：集成规划，行动和学习
--------------------------------

当规划在线完成时，在与环境交互时，会出现许多有趣的问题。
从交互中获得的新信息可能会改变模型，从而与规划相互作用。
可能需要以某种方式定制规划过程以适应当前正在考虑或在不久的将来预期的状态或决定。
如果决策制定和模型学习都是计算密集型过程，则可能需要在它们之间划分可用的计算资源。
为了开始探索这些问题，我们在本节中介绍了Dyna-Q，这是一个集成了在线规划个体所需的主要函数的简单架构。
每个函数都以简单，几乎无关紧要的形式出现在Dyna-Q中。
在后面的章节中，我们详细介绍了实现每个函数的一些替代方法以及它们之间的交易。
目前，我们只是为了说明这些想法并激发你的直觉。

.. figure:: images/relationships-between-experience-model-values-and-policy.png
    :align: right
    :width: 250px

在规划个体中，实际经验至少有两个角色：它可以用于改进模型（使其更准确地匹配真实环境），
并且它可以用来直接改进价值函数和政策，使用前面章节中讨论的各种强化学习方法。
前者称为模型学习，后者称为 *直接强化学习* （直接RL）。
右图中总结了经验，模型，价值和策略之间可能存在的关系。
每个箭头显示影响和推测改善的关系。请注意，经验如何通过模型直接或间接地改进价值函数和策略。
后者，有时被称为 *间接强化学习*，参与规划。

直接和间接方法都有优点和缺点。间接方法通常可以更充分地利用有限的经验，从而以更少的环境互动实现更好的策略。
另一方面，直接方法更简单，并且不受模型设计中的偏差影响。
有些人认为间接方法总是优于直接方法，而另一些方法则认为直接方法是大多数人类和动物学习的原因。
心理学和人工智能方面的相关辩论关心认知的相对重要性而不是试错学习，以及审议规划而不是被动决策
（参见第14章，从心理学的角度讨论其中的一些问题）。
我们的观点是，所有这些辩论中的替代方案之间的对比被夸大了，通过认识到这两方之间的相似性而不是反对它们，可以获得更多的洞察力。
例如，在本书中，我们强调了动态规划和时序差分方法之间的深刻相似性，即使一个是为规划而另一个是为不基于模型学习而设计的。

Dyna-Q包括上图中显示的所有过程──规划，行为，模型学习和直接RL──所有持续发生的过程。
规划方法是上一节的随机样本一步表格Q-planning方法。直接RL方法是一步表格Q-learning。
模型学习方法也是基于表格的，并假设环境是确定性的。
每次转换 :math:`S_{t}, A_{t} \rightarrow R_{t+1}, S_{t+1}` 之后，
模型在表格条目中记录 :math:`S_{t}, A_{t}` 的预测，:math:`S_{t+1}, A_{t+1}` 将确定性地跟随。
因此，如果使用之前经历过的状态-动作对查询模型，则它仅返回最后观察到的下一状态和下一个奖励作为其预测。
在规划期间，Q-planning算法仅从先前已经经历过的状态-动作对中随机采样（在步骤1中），
因此永远不会使用它没有信息的状态-动作对来查询模型。

Dyna个体的整体架构，其中Dyna-Q算法就是一个例子，如图8.1所示。
中间栏代表了个体与环境之间的基本互动，从而产生了真实经验的轨迹。
图左侧的箭头表示根据实际经验进行的直接强化学习，以改善价值函数和策略。
右边是基于模型的流程。该模型从实际经验中学习，并产生模拟经验。
我们使用术语 *搜索控件* 来指代为模型生成的模拟经验选择起始状态和动作的过程。
最后，通过将强化学习方法应用于模拟经验来实现规划，就像它们确实发生过一样。
通常，如在Dyna-Q中，相同的强化学习方法既用于从真实经验中学习，也用于从模拟经验中进行规划。
因此，强化学习方法是学习和计划的“最终共同路径”。
学习和规划是深深融合的，因为他们分享几乎所有相同的机制，只区别于他们的经验来源。

.. figure:: images/figure-8.1.png

    **图8.1：** 一般Dyna架构。在环境和策略之间来回传递的真实经验会影响策略和价值函数，
    其方式与环境模型生成的模拟经验非常相似。

从概念上讲，规划，行动，模型学习和直接RL在Dyna个体中同时并行地发生。
但是，为了在串行计算机上具体化和实现，我们完全指定它们在一个时间步长内发生的顺序。
在Dyna-Q中，行动，模型学习和直接RL过程几乎不需要计算，我们假设它们只消耗了一小部分时间。
每个步骤中的剩余时间可用于规划过程，这本身就是计算密集型过程。
我们假设在行动，模型学习和直接RL之后的每个步骤中有时间来完成Q-plainning算法的n次迭代（步骤1-3）。
在下面框中的Dyna-Q的伪代码算法中，
:math:`Model(s,a)` 表示状态-动作对 :math:`(s,a)` 的（预测的下一状态和奖励）的内容。
直接强化学习，模型学习和规划分别通过步骤（d），（e）和（f）实施。如果省略（e）和（f），则剩余算法将是一步表格Q-learning。

.. admonition:: 表格Dyna-Q
    :class: important

    对 :math:`s\in\mathcal{S}` 和 :math:`a\in\mathcal{A}`，初始化 :math:`Q(s,a)` 和 :math:`Model(s,a)`

    一直循环：

        （a） :math:`S \leftarrow` 当前（非终端）状态

        （b） :math:`A \leftarrow \varepsilon\text{-贪婪}(S, Q)`

        （c）执行动作 :math:`A`；观察结果奖励 :math:`R` 和状态 :math:`S^{\prime}`

        （d） :math:`Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right]`

        （e） :math:`Model(S, A) \leftarrow R, S^{\prime}` （假设确定性环境）

        （f）循环 :math:`n` 次：

            :math:`S \leftarrow` 随机先前观察到的状态

            :math:`A \leftarrow` 先前在 :math:`S` 中采取的随机动作

            :math:`R, S^{\prime} \leftarrow Model(S, A)`

            :math:`Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right]`



8.3 当模型错误时
--------------------


8.4 优先扫描
------------------


8.5 预期与样本更新
---------------------


8.6 轨迹采样
-----------------


8.7 实时动态规划
---------------------


8.8 决策时规划
-----------------


8.9 启发式搜索
----------------


8.10 推出（Rollout）算法
------------------------------


8.11 蒙特卡洛树搜索
---------------------


8.12 本章总结
---------------


8.13 第一部分摘要：维度（Dimensions）
----------------------------------------


书目和历史评论
---------------
