第8章 表格方法规划和学习
========================

在本章中，我们开发了强化学习方法的统一视图，这些方法需要环境模型，例如动态规划和启发式搜索，
以及可以在没有模型的情况下使用的方法，例如蒙特卡罗和时序差分方法。
这些分别称为 *基于模型（model-based）* 和 *不基于模型（model-free）*的强化学习方法。
基于模型的方法依赖于 *规划（planning）* 作为其主要组成部分，而不基于模型方法主要依赖于 *学习（learning）*。
虽然这两种方法之间存在着真正的差异，但也存在很大的相似之处。
特别是，这两种方法的核心是价值函数的计算。此外，所有方法都基于展望未来事件，计算备份值，然后将其用作近似值函数的更新目标。
在本书的前面，我们将蒙特卡洛和时序差分方法作为不同的替代方法，然后展示了如何通过n步方法统一它们。
我们在本章中的目标是基于模型和不基于模型方法的类似集成。
在前面的章节中已经确定了这些不同之处，我们现在探讨它们可以混合的程度。


8.1 模型和规划
-----------------

通过环境 *模型*，我们指的是个体可以用来预测环境如何响应其行为的任何事物。
给定状态和动作，模型产生对结果下一状态和下一奖励的预测。
如果模型是随机的，那么有几种可能的下一个状态和下一个奖励，每个都有一定的发生概率。
一些模型描述了所有可能性及其概率；这些我们称之为 *分布模型*。
其他模型只产生一种可能性，根据概率进行采样；这些我们称之为 *采样模型*。
例如，考虑对十几个骰子的总和进行建模。
分布模型将产生所有可能的总和及其发生的概率，而样本模型将产生根据该概率分布绘制的单个总和。
在动态规划中假设的模型──MDP动力学的估计 :math:`p\left(s^{\prime}, r | s, a\right)` ──是分布模型。
第5章中二十一点示例中使用的模型是一个采样模型。分布模型比样本模型更强大，因为它们可以始终用于产生样本。
但是，在许多应用中，获取样本模型比分布模型容易得多。十几个骰子就是一个简单的例子。
编写一个计算机程序来模拟骰子滚动并返回总和是很容易的，但是找出所有可能的总和及其概率更难且更容易出错。

模型可用于模仿或模拟体验。给定起始状态和动作，样本模型产生可能的转换，分布模型生成由其发生概率加权的所有可能转换。
给定起始状态和策略，样本模型可以产生整个回合，分布模型可以生成所有可能的回合及其概率。
在这两种情况下，我们都说模型用于 *模拟* 环境并产生 *模拟经验*。

在不同领域中，*规划* 这个词以几种不同的方式使用。
我们使用该术语来指代将模型作为输入并生成或改进与建模环境交互的策略的任何计算过程：

.. math::

    \text{模型} \stackrel{规划}{\longrightarrow} \text{策略}

在人工智能中，根据我们的定义，有两种不同的规划方法。*状态空间规划*，包括我们在本书中采用的方法，
主要被视为通过状态空间搜索最优策略或目标的最佳路径。动作导致从状态到状态的转换，并且价值函数在状态上计算。
在我们所说的 *规划空间规划* 中，规划是通过规划空间进行搜索。
操作将一个规划转换为另一个规划，并在规划空间中定义价值函数（如果有）。
规划空间规划包括演化方法和“偏序规划”，这是人工智能中的一种常见规划，其中步骤的排序在规划的所有阶段都没有完全确定。
规划空间方法难以有效地应用于强化学习中关注的随机序列决策问题，我们不会进一步考虑它们（但参见，例如，Russell和Norvig，2010）。

我们在本章中提出的统一观点是，所有的状态空间规划方法都有一个共同的结构，这种结构也存在于本书所介绍的学习方法中。
本章的其余部分将开发此视图，但有两个基本思想：
（1）所有状态空间规划方法都将计算价值函数作为改进策略的关键中间步骤，
（2）它们通过应用于模拟经验的更新或备份操作计算价值函数。
这种通用结构可以表示如下：

.. math::

    \text{模型} \longrightarrow \text{模拟经验} \stackrel{备份}{longrightarrow} \text{价值} \longrightarrow \text{策略}

动态规划方法显然适合这种结构：它们扫描状态空间，为每个状态生成可能的转换分布。
然后，每个分布用于计算备份值（更新目标）并更新状态的估计值。
在本章中，我们认为各种其他状态空间规划方法也适合这种结构，
各种方法的区别仅在于它们的更新类型，它们的执行顺序以及备份信息的保留时间。

以这种方式查看规划方法强调了它们与我们在本书中描述的学习方法的关系。
学习和规划方法的核心是通过备份更新操作来估计价值函数。
不同之处在于，规划使用模型生成的模拟经验，而学习方法则使用环境产生的实际经验。
当然，这种差异导致了许多其他差异，例如，如何评估性能以及如何灵活地产生经验。
但是，共同的结构意味着许多想法和算法可以在规划和学习之间转移。
特别地，在许多情况下，学习算法可以代替规划方法的关键更新步骤。
学习方法仅需要经验作为输入，并且在许多情况下，它们可以应用于模拟经验以及真实经验。
下面的框显示了基于一步表格Q-learning的规划方法和来自样本模型的随机样本的简单示例。
这种方法，我们称之为 *随机样本一步表格Q-planning*，
在一步表格Q-learning收敛于真实环境的最优政策的相同条件下，收敛到模型的最优策略
（必须在步骤1中无限次地选择每个状态-动作，并且 :math:`\alpha` 必须随时间适当减小）。

.. admonition:: 随机样本一步表格Q-planning
    :class: important

    一直循环：

        1. 随机选择一个状态 :math:`S\in\mathcal{S}` 和动作 :math:`A\in\mathcal{A}`

        2. 将 :math:`S, A` 发送到样本模型，并获得样本下一个奖励 :math:`R` 和样本下一个状态 :math:`S^{\prime}`

        3. 对 :math:`S,A,R,S^{\prime}` 应用一步表格Q-learning：

            :math:`Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right]`

除了统一的规划和学习方法之外，本章的第二个主题是以小的渐进步骤进行规划的好处。
这使规划能够在几乎没有浪费的计算的情况下随时中断或重定向，这似乎是将计划与行为和学习模型有效混合的关键要求。
如果问题太大而无法准确解决，即使在纯粹的规划问题上，以非常小的步骤进行规划也可能是最有效的方法。


8.2 Dyna：综合规划，行动和学习
--------------------------------


8.3 当模型错误时
--------------------


8.4 优先扫描
------------------


8.5 预期与样本更新
---------------------


8.6 轨迹采样
-----------------


8.7 实时动态规划
---------------------


8.8 决策时规划
-----------------


8.9 启发式搜索
----------------


8.10 推出（Rollout）算法
------------------------------


8.11 蒙特卡洛树搜索
---------------------


8.12 本章总结
---------------


8.13 第一部分摘要：维度（Dimensions）
----------------------------------------


书目和历史评论
---------------
