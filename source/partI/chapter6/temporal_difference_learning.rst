第6章 时序差分学习
==================

如果必须将一个想法确定为强化学习的核心和新颖，那么毫无疑问它将是 *时序差分* （TD）学习。
TD学习是蒙特卡洛思想和动态规划（DP）思想的结合。与蒙特卡洛方法一样，TD方法可以直接从原始经验中学习，而无需环境动态模型。
与DP一样，TD方法部分基于其他学习估计更新估计，而无需等待最终结果（它们是自举）。
TD，DP和蒙特卡洛方法之间的关系是强化学习理论中反复出现的主题；本章是我们开始探索这个关系。
在我们完成之前，我们将看到这些想法和方法相互融合，并且可以以多种方式组合。
特别是，在第7章中，我们介绍了n步算法，它提供了从TD到蒙特卡洛方法的桥梁，
在第12章中我们介绍了 TD(:math:`\lambda`) 算法，它无缝地统一了它们。

像往常一样，我们首先关注策略评估或 *预测* 问题，即估算给定策略 :math:`\pi` 的价值函数 :math:`v_\pi` 的问题。
对于控制问题（找到最优策略），DP、TD和蒙特卡洛方法都使用广义策略迭代（GPI）的一些变体。
方法的差异主要在于它们对预测问题的方法的差异。


6.1 TD预测
-------------


6.2 TD预测方法的优势
---------------------


6.3 TD(0)的最优
------------------


6.4 Sarsa：在策略TD控制
------------------------


6.5 Q-学习：离策略TD控制
-----------------------------


6.6 预期的Sarsa
---------------


6.8 最大化偏差和双重学习
-------------------------


6.9 游戏，后遗症和其他特殊情况
------------------------------


6.10 总结
-----------


书目和历史评论
---------------

