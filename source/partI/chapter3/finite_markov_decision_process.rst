第3章 有限马尔可夫决策过程
==========================

在本章中，我们将介绍有限马尔可夫决策过程或有限MDP的形式问题，我们将在本书的其余部分尝试解决这些问题。
这个问题涉及评价反馈，如在老虎机问题中，但也涉及一个关联方面，即在不同情况下选择不同的行动。
MDP是顺序决策的经典形式化，其中行动不仅影响直接奖励，还影响后续情况或状态，以及贯穿未来的奖励。
因此，MDP涉及延迟奖励以及交换即时与延迟奖励的需要。
在老虎机问题中，我们估计每个动作a的价值 :math:`q_*(a)`，
在MDP中我们估计每个状态s中每个动作a的价值 :math:`q_*(s, a)`，
或者我们估计给出最佳行动选择的每个状态的价值 :math:`v_*(s)`。
这些依赖于状态的量对于准确地为个人行动选择的长期结果分配信用至关重要。

MDP是强化学习问题的数学理想化形式，可以对其进行精确的理论陈述。
我们介绍问题数学结构的关键元素，如回归，值函数和Bellman方程。
我们试图传达可能被用作有限MDP的各种可能的应用程序。
与所有人工智能一样，在适用范围和数学易处理性之间存在着一种矛盾。
在本章中，我们将介绍这种矛盾关系，并讨论它所暗示的一些权衡和挑战。
第17章介绍了在MDP之外进行强化学习的一些方法。

3.1 个体环境接口
^^^^^^^^^^^^^^^^^^
​
MDP旨在直接构建从交互中学习以实现目标的问题。
学习者和决策者被称为 *个体（agent）*。它与之交互的东西，包括个体之外的所有东西，被称为 *环境*。
这些交互持续不断，个体选择动作同时环境响应那些动作并向个体呈现新情况 [1]_。
环境还产生奖励，这是个体通过其行动选择寻求最大化的特殊数值。

.. figure:: images/figure-3.1.png

    图3.1：马尔可夫决策过程中的个体 - 环境交互。

更具体地，个体和环境在离散时间序列每一步相互作用，:math:`t = 0,1,2,3,\dots` [2]_。
在每个时间步t，个体接收环境 *状态* :math:`S_{t} \in \mathcal{S}` ，
并在此基础上选择一个 *动作*，:math:`A_{t}\in \mathcal{S}(s)` [3]_。
每一步后，作为它行动的结果，个体接收到一个 *奖励* 值，
:math:`R_{t+1} \in \mathcal{R} \subset \mathbb{R}`，
并且自身处于一个新状态，:math:`S_{t+1}` [4]_ ，
MDP和个体一起产生了一个如下所示的序列或 *轨迹*：

.. math::
    :label: 3.1

    S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,\dots

在 *有限* MDP中，状态，动作和奖励
（:math:`\mathcal{S}`，:math:`\mathcal{A}` 和 :math:`\mathcal{R}`）的集合都具有有限数量的元素。
在这种情况下，随机变量 :math:`R_t` 和 :math:`S_t` 具有明确定义的离散概率分布，仅取决于先前的状态和动作。
也就是说，对于这些随机变量的特定值，:math:`s^\prime \in \mathcal{S}` 和 :math:`r \in \mathcal{R}`，
在给定前一状态和动作的特定值的情况下，存在这些值在时间t发生的概率：

.. math::
    :label: 3.2

    p(s^\prime,r|s,a) \doteq Pr\{S_t=s^\prime,R_t=r|S_{t-1}=s,R_{t-1}=a\}

对所有 :math:`s^\prime, s \in \mathcal{S}`，:math:`\mathcal{r} \in \mathcal{R}`
和 :math:`a \in \mathcal{A}(s)`。函数 :math:`p` 定义了MDP的 *动态*。
方程中等号上的点提醒我们它是一个定义（在这个例子中是函数 :math:`p`），而不是从先前定义得出的事实。
动力学函数 :math:`p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \to [0, 1]`
是四个参数的普通确定性函数。 中间的“|”来自条件概率的符号，
但这里只是提醒我们 :math:`p` 指定 :math:`s` 和 :math:`a` 的每个选择的概率分布，即

.. math::
    :label: 3.3

    \sum_{s^\prime \in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s^\prime,r|s,a)=1，对所有 s \in \mathcal{S}，a \in \mathcal{A}(s)

在 *马尔可夫* 决策过程中，:math:`p` 给出的概率完全表征了环境的动态。
也就是说，:math:`S_t` 和 :math:`R_t` 的每个可能值的概率
仅取决于前一个状态和动作 :math:`S_{t-1}` 和 :math:`A_{t-1}`，
并且在给定它们的情况下，它们根本不依赖于先前的状态和动作。
最好将其视为对决策过程的限制，而不是对 *状态* 的限制。
状态必须包括有关过去的个体-环境交互的所有方面的信息，这些信息对未来有所影响。
如果确实如此，那么就说该状态拥有 *马尔可夫性*。我们将在本书中假设马尔可夫性，
尽管从第二部分开始我们将考虑不依赖它的近似方法，并且在第17章中我们考虑如何从非马尔可夫观察中学习和构造马尔可夫状态。

从四参数动力学函数p中，可以计算出人们可能想知道的关于环境的任何其他信息，例如状态转移概率（我们将其略微滥用符号表示为三参数函数
:math:`p : \mathcal{S} \times \mathcal{S} \times \mathcal{A} \to [0, 1]`），

.. math::
    :label: 3.4

    p(s^\prime|s,a) \doteq Pr\{S_t=s^\prime|S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in\mathcal{R}}p(s^\prime,r|s,a)

我们还可以将状态 - 动作对的预期奖励计算为双参数函数 :math:`r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}`，

.. math::
    :label: 3.5

    r(s,a)\doteq\mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in\mathcal{R}}r\sum_{s^\prime\in\mathcal{S}}p(s^\prime,r|s,a)

以及状态 - 行动 - 下一状态三元组的预期奖励作为三个参数函数
:math:`r : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \mathbb{R}`，

.. math::
    :label: 3.6

    r(s,a,s^\prime)\doteq\mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s^\prime\right]=\sum_{r\in\mathcal{R}}r\frac{p(s^\prime,r|s,a)}{p(s^\prime|s,a)}

在本书中，我们通常使用四参数p函数（3.2），但这些其他符号中的每一个偶尔也很方便。

MDP框架是抽象和灵活的，可以以不同的方式应用在很多不同的问题上。
例如，时间步长不需要指固定的实时间隔；它们可以指任意连续的决策和行动阶段。
这些动作可以是低级控制，例如施加到机器人手臂的电动机的电压，或高级​​决策，例如是否要吃午餐或进入研究生院。
同样，状态也可以采取各种各样的形式。它们可以完全由低级感觉决定，例如直接传感器读数，
或者它们可以更高级和抽象，例如房间中物体的符号描述。
可以基于对过去的感觉的记忆，甚至是完全精神的或主观的来构成一个状态。
例如，个体可能处于不确定对象在哪里的状态，或者在某些明确定义的意义上感到惊讶的状态。
同样，某些行为可能完全是精神上的或可计算的。例如，某些操作可能会控制代理选择考虑的内容，或者它关注的重点。
一般而言，行动可以是我们想要学习如何制定的任何决定，而状态可以是我们可以知道的任何可能有助于制作它们的任何事物。

特别是，个体和环境之间的边界通常与机器人或动物身体的物理边界不同。
通常，边界更接近于个体。例如，机器人及其传感硬件的电动机和机械联动件通常应被视为环境的一部分而不是个体的一部分。
同样，如果我们将MDP框架应用于人或动物，肌肉，骨骼和感觉器官应被视为环境的一部分。
也许，奖励可以在自然和人工学习系统的物理体内计算，但被认为是个体的外部。​

我们遵循的一般规则是，任何不能被个体任意改变的东西都被认为是在它之外，因此也是其环境的一部分。
我们不假定个体对环境一无所知。例如，个体通常非常了解如何根据其动作及其所处的状态来计算奖励。
但是我们总是认为奖励计算是在个体之外的，因为它是根据个体的任务所定义的，因此不能由个体来随意改变。
事实上，在某些情况下，个体就算知道它的环境是如何运行的，并且仍然面临着艰难的强化学习任务，
正如我们可以知道一个魔方是如何运行的，但仍然无法解开它。
个体-环境的边界代表着对个体的绝对控制能力的限制，而不是限制它的知识。

个体-环境的边界可以位于不同的地方以用于不同的目的。在复杂的机器人中，许多不同的个体可能同时运行，每个个体都有自己的边界。
例如，一个个体可以做出高级决策，高级决策可由低级个体面临的状态组成，从而实现高层次的决策。
在实践中，一旦选择了特定的状态，动作和奖励，就确定个体-环境边界，从而确定了感兴趣的特定决策制定任务。

MDP框架是从相互作用的目标导向学习的问题中抽象出来的。
它提出无论传感，记忆和控制装置的细节，以及任何目标试图达到的目标，
学习目标导向行为的任何问题都可以减少为个体及其环境之间来回传递的三个信号：
一个信号表示个体做出的选择（动作），一个信号表示作出选择的基础（状态），以及另一个信号来定义个体的目标（奖励）。
这个框架可能不足以有效地代表所有决策学习问题，但它已被证明是广泛有用和适用的。

当然，特定的状态和操作因任务而异，并且它们的表示方式会对性能产生很大影响。
在强化学习中，与其他类型的学习一样，这种表征性选择目前更多的是艺术而非科学。
在本书中，我们提供了一些关于表达状态和行为的好方法的建议和例子，但我们主要焦点的是一旦表示被确定，如何学习行为的一般原则。

**例3.1：生物反应器** 假设强化学习用于确定生物反应器（用于生产有用化学品的大量营养物和细菌）的瞬间温度温度和搅拌速率。
这种应用中的动作可以是传递到下级控制系统的目标温度和目标搅拌速率，该控制系统又直接激活加热元件和马达以实现目标。
状态可能是有可能被过滤和延迟热电偶和其他传感器读数，加上代表大桶和目标化学品成分的符号输入。
奖励可能是生物反应器产生有用化学品的速率的逐时测量。
请注意，此处每个状态都是传感器读数和符号输入的列表或矢量，每个动作都是由目标温度和搅拌速率组成的矢量。
强化学习任务的典型特征是具有这种结构化表示的状态和动作。另一方面，奖励总是单个数字。

**例3.2：拾取和放置机器人** 考虑使用强化学习来控制机器人手臂在重复拾取和放置任务中的运动。
如果我们想要学习快速和平稳的运动，则当前个体将必须直接控制马达并且具有关于机械联动装置的当前位置和速度的低延迟信息。
在这种情况下的动作可能是每个关节处施加到每个电动机的电压，并且状态可能是关节角度和速度的最新读数。
对于成功拾取和放置的每个对象，奖励可能为+1。为了鼓励平稳移动，在每个时间步骤上，可以根据动作的瞬间“急动”给出小的负面奖励。

*练习3.1* 设计适合MDP框架的三个自己的示例任务，为每个任务确定其状态，动作和奖励。
尽可能使这三个例子彼此 *不同*。该框架是抽象和灵活的，可以以多种不同的方式应用。在至少一个示例中以某种方式扩展其限制。

*练习3.2* MDP框架是否足以有效地代表 *所有* 目标导向的学习任务？你能想到任何明显的例外吗？

*练习3.3* 考虑驾驶问题。你可以根据加速器，方向盘和制动器（即你的身体与机器接触的位置）来定义动作。
或者你可以将它们定义得更远，比如橡胶与道路相遇，考虑你的动作是轮胎扭矩。
或者你可以进一步定义它们，比如说，你的大脑掌控身体，肌肉抽搐的动作来控制你的四肢。
或者你可以达到一个更高的层次，说你的行动是你选择开车的地方。
什么是个体和环境之间合适的层次和位置分界？在什么基础上，该线的一个位置优先于另一个？
是否有任何根本原因选择一个位置而不是另一个位置，还是随意选择？

.. admonition:: 例3.3：环保机器人
    :class: important

    移动机器人的工作是在办公室环境中收集空的汽水罐。它有用于检测汽水罐的传感器，以及可以将它们拾起并放置在机箱中的臂和夹具；它使用可充电电池供电。
    机器人的控制系统具有用于解释传感器信息，用于导航以及用于控制手臂和夹具的部件。
    关于如何搜索汽水罐的高级决策是由强化学习个体根据电池的当前充电水平做出的。
    举一个简单的例子，我们假设只能区分两个电荷电平，包括一个小的状态集 :math:`\mathcal{S}=\{高，低\}`。
    在每个状态，个体可以决定是否（1）在一段时间内主动 **搜索** 汽水罐，（2）保持静止并 **等待** 某人给它汽水罐，或（3）返回其本垒为电池 **充电**。
    当能量水平很 **高** 时，充电总是愚蠢的，所以我们不会将其包含在为此状态设定的动作中。
    动作集是 :math:`\mathcal{A}(高)=\{搜索, 等待\}` 和 :math:`\mathcal{A}(低)=\{搜索, 等待, 充电\}`。

    奖励在大多数情况下为零，但是当机器人固定空罐时变为正值，或者如果电池完全耗尽则变为负值。
    找到汽水罐的最好方法是主动搜索它们，但这会耗尽机器人的电池电量，而等待则不会。
    每当机器人正在搜索时，存在其电池耗尽的可能性。在这种情况下，机器人必须关闭并等待获救（产生低回报）。
    如果电池电量水平 **高**，则可以始终完成一段主动搜索而没有耗尽电池的风险。
    以 **高** 电量水平开始的搜索周期使电量水平以概率 :math:`\alpha` 保持并且以概率 :math:`1-\alpha` 降低至 **低** 电量水平。
    另一方面，当电量水平 **低** 时进行的搜索周期使其以概率 :math:`\beta` 变 **低** 并且以概率 :math:`1-\beta` 消耗电池。
    在后一种情况下，必须拯救机器人，然后将电池重新充电至 **高** 电量水平。
    机器人收集的每个汽水罐都可以作为单位奖励计算，而每当机器人必须获救时，奖励为-3。
    用 :math:`r_{搜索}` 和 :math:`r_{等待}`，其中 :math:`r_{搜索}>r_{等待}`，分别表示机器人在搜索和等待时将收集的预期罐数（以及预期的奖励）。
    最后，假设在跑步回家期间不能收集罐头，并且在电池耗尽的过程中不能收集罐头。
    这个系统是一个有限的MDP，我们可以记下转移概率和预期的奖励，动态如左表所示：

    .. figure:: images/table-figure.png

    请注意，表中有一行代表当前状态 :math:`s`，动作 :math:`a`，:math:`a\in\mathcal{A}(s)`和下一个状态 :math:`s` 的每种可能组合。
    某些转换的概率为零，因此没有为它们指定预期的奖励。右侧所示是另一种有用的方法，可以总结有限MDP的动态，称为 *转换图*。
    有两种节点：*状态节点* 和 *动作节点*。每个可能的状态都有一个状态节点（由状态名称标记的大圆圈），
    以及每个状态-动作对的动作节点（由行动名称标记并由线连接的小实心圆圈）。
    从状态 :math:`s` 开始并采取动作 :math:`a`，你将沿着从状态节点 :math:`s` 到动作节点 :math:`(s,a)` 的线路移动。
    然后，环境通过离开动作节点 :math:`(s,a)` 的箭头之一转换到下一个状态的节点。
    每个箭头对应一个三元组 :math:`(s,s^\prime,a)`，其中 :math:`s^\prime` 是下一个状态，我们用转移概率 :math:`p(s^\prime|s,a)` 标记箭头，
    以及该转换的预期回报 :math:`r(s,a,s^\prime)`。请注意，标记离开动作节点的箭头的转移概率和总是为1。

*练习3.4* 给出一个类似于例3.3中的表，但是对于 :math:`p(s^\prime,r|s,a)`。
它应该有 :math:`s, a, s^\prime, r` 和 :math:`p(s^\prime,r|s,a)` 的列，
以及 :math:`p(s^\prime,r|s,a)>0` 的每个4元组的行。

3.2 目标和奖励
^^^^^^^^^^^^^^

在强化学习中，个体的目的或目标被形式化为从环境传递到个体的特殊信号（称为 *奖励*）。
在每个时间步骤，奖励是一个简单的数字，:math:`R_{t} \in \mathbb{R}`。非正式地，个体的目标是最大化其收到的总奖励。
这意味着最大化不是立即奖励，而是长期累积奖励。我们可以用 * 奖励假说* 来清楚表达这个非正式的想法：

> 所有我们所说的目标和目的都可以被认为是所接收的标量信号（称为奖励）的累积和的预期值的最大化。

使用奖励信号来形式化目标的想法是强化学习的最显着特征之一。

尽管根据奖励信号制定目标可能最初看起来有限，但在实践中它已被证明是可行的和广泛适用的。
看到这一点的最佳方法是考虑如何使用或可能使用它的示例。例如，为了让机器人学会走路，研究人员在每个时间步长上提供了与机器人向前运动成比例的奖励。
在让机器人学会如何逃离迷宫时，在逃脱前经过的每一步的奖励通常为-1；这鼓励代理人尽快逃脱。
为了让机器人学会找到并收集空的汽水罐进行回收利用，人们可能会在大多数情况下给予奖励零，然后每收集一次空罐给+1的回报。
人们可能还想在机器人碰到东西或者有人大喊大叫时给予机器人负面的奖励。
对于学习玩跳棋或国际象棋的代理人来说，获胜的自然奖励为+1，失败为-1，绘图和所有非终结位置为0。

您可以看到所有这些示例中发生的情况。个体总是学会最大化其奖励。
如果我们希望它为我们做点什么，我们必须以这样的方式为它提供奖励，即在最大化它们的同时，个体也将实现我们的目标。
因此，我们建立的奖励真正表明我们想要实现的目标至关重要。
特别是，奖励信号不是向个体提供关于 *如何* 实现我们想要做的事情的先验知识的地方 [5]_。
例如，国际象棋游戏个体应该仅仅因为实际获胜而获得奖励，而不是为了实现拿走对手的棋子或控制棋盘中心这样的子目标。
如果实现这些类型的子目标得到奖励，那么个体可能会找到一种方法来实现它们而不实现真正的目标。
例如，即使以失去游戏为代价，它也可能找到一种方法来获取对手的棋子。奖励信号是你与机器人沟通的 *方式*，而不是您希望 *如何* 实现 [6]_。

3.3 回报和情节
^^^^^^^^^^^^^^^^

到目前为止，我们已经讨论了非正式学习的目标。我们已经说过，个体的目标是获得从长远来看的最大累积奖励。
这如何正式定义？如果在时间步骤t之后接收的奖励序列表示为 :math:`R_{t + 1}, R_{t + 2}, R_{t + 3}, \dots`，
那么，我们希望最大化这个序列的具体什么方面？一般而言，我们寻求最大化 *预期收益*，其中收益 :math:`G_{t}` 被定义为奖励序列的某个特定函数。
在最简单的情况下，回报是奖励的总和：

.. math::
    :label: 3.7

    G_{t} \doteq R_{t+1} +R_{t+2} + R_{t+3} + \dots + R_{T}，

其中T是最后一步。这种方法在存在最终时间步骤的自然概念的应用中是有意义的，
也就是说，当个体-环境交互自然地分解为子序列时，我们称之为 *情节* [7]_，例如玩游戏，穿过迷宫，或任何形式的重复互动。
每个情节在称为 *终点* 状态的特殊状态结束，随后是重置到标准起始状态或从起始状态的标准分布的抽样。
即使你认为情节以不同的方式结束，例如输赢游戏，下一情节的开始也与上一情节的结束无关。
因此，所有这些情节都可以被认为是以相同的终点状态结束，对不同的结果有不同的奖励。具有这种情节的任务被称为 *情节任务*。
在情节任务中，我们有时需要将所有非终结状态的集合，表示为 :math:`\mathcal{S}` ，
与所有状态的集合加上终端状态，表示为 :math:`\mathcal{S^+}` ，区分开来。
终止时间T是随机变量，从情节到情节通常不同。

另一方面，在许多情况下，个体-环境交互不会自然地分解为可识别的事件，而是持续不断地进行。
例如，这将是一个自然的方式来制定一个持续的过程控制任务，或具有长寿命的机器人上的应用。我们将这些称之为 *持续任务*。
回报公式（3.7）对于连续的任务是有问题的，因为最终时间步长将是 :math:`T=\infty`，并且返回，这是我们试图最大化的，本身可以很容易是无限的。
（例如，假设个体在每个时间步都获得+1的奖励。）因此，在本书中，我们通常使用返回的定义，在概念上稍微复杂但在数学上更简单。

我们需要的另一个概念是 *衰减因子*。根据这种方法，个体尝试选择动作，以使其在未来接收的衰减的奖励的总和最大化。
特别是，它选择 :math:`A_{t}` 来最大化预期的 *衰减回报*：

.. math::
    :label: 3.8

    G_{t} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}

其中 :math:`\gamma` 是参数，:math:`0 \leq\gamma \leq 1`，称为 *衰减因子*。

衰减率决定了未来奖励的现值：未来收到的k个时间步骤的奖励价值仅为立即收到的 :math:`\gamma^{k-1}` 倍。
如果奖励是立即被接收的则是值得的。
如果 :math:`\gamma < 1`，只要奖励序列 :math:`\{R_{k}\}` 有界，则（3.8）中的无限和具有有限值。
如果 :math:`\gamma = 0`，个体是“短视”的，只关注最大化立即奖励：
在这种情况下，其目标是学习如何选择 :math:`A_{t}` 以使 :math:`R_{t+1}` 最大化。
如果每个个体的行为恰好只影响即时奖励，而不影响未来的奖励，那么短视个体可以通过单独最大化每个即时奖励来最大化（3.8）。
但一般来说，最大化立即奖励的行为可以减少对未来奖励的获取，从而减少回报。
当 :math:`\gamma` 接近1时，回报目标更加强烈地考虑了未来的回报；个体变得更有远见。

连续时间步骤的回报以一种对强化学习的理论和算法很重要的方式相互关联：

.. math::
    :label: 3.9

    \begin{align*}
    G_{t} &\doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots \\
    &= R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \dots) \\
    &= R_{t+1} + \gamma G_{t+1}
    \end{align*}

请注意，这适用于所有时间步骤 :math:`t<T`，即使终止发生在 :math:`t+1`，如果我们定义 :math:`G_T=0`，也是适用的。
这通常可以很容易地计算奖励序列的回报。

请注意，尽管回报公式（3.8）是无穷多个项的总和，但奖励非零并且为常数，如果 :math:`\gamma < 1`，则它仍然是有限的。
例如，如果奖励是常数+1，那么返回是

.. math::
    :label: 3.10

    G_t = \sum_{k=0}^{\infty}\gamma^k = \frac{1}{1-\gamma}

*练习3.5* 3.1节中的等式是针对连续的情况，需要进行修改（非常轻微）以应用于情节任务。通过给出（3.3）的修改版本，表明你知道所需的修改。

.. figure:: images/figure-3.2.png
   :alt: figure3.2
   :align: right

**示例3.4：杆平衡** 这项任务的目的是将力施加到沿着轨道移动的推车上，以便保持铰接在推车上的杆不会翻倒：
如果杆从垂直方向落下一个给定角度或者如果推车超过给定角度，则会发生故障离开赛道。
每次故障后，极点都会重置为垂直。这个任务可以被视为偶发事件，其中自然事件是重复平衡极点的尝试。
在这种情况下，对于没有发生故障的每个时间步骤，奖励可以是+1，因此每次返回将是直到失败的步骤数。
在这种情况下，永远成功的平衡将意味着无限的回报。或者，我们可以使用衰减将极点平衡视为一项持续性任务。
在这种情况下，每次失败时奖励为1，其他时间奖励为零。然后每次返回 :math:`1-\gamma^K` 与K相关，其中K是失败前的时间步数。
在任何一种情况下，通过尽可能长时间保持极点平衡来使回报最大化。

*练习3.5* 假设你将杆平衡作为一个情节性任务，但是也使用了衰减因子，除了-1是失败之外，所有奖励都是零。
那么每次回报是多少？这个回报与有衰减的持续任务有什么不同？

*练习3.7* 想象一下，你正在设计一个运行迷宫的机器人。你决定在逃离迷宫时奖励+1，在其他时候奖励零。
任务似乎自然地分解为情景，即连续贯穿迷宫的运行，所以你决定把它当作一个偶然的任务，其目标是最大化预期的总奖励（3.7）。
运行学习个体一段时间后，您会发现它从迷宫中逃脱没有任何改善。出了什么问题？你是否有意识地向个体传达了你希望它实现的目标？

*练习3.8* 假设 :math:`\gamma=0.5` 并且接收以下奖励序列 :math:`R_1=1`，:math:`R_2=2`，
:math:`R_3=6`，:math:`R_4=3`，并且 :math:`R_5=2`，其中 :math:`T=5`。
:math:`G_0，G_1，\dots，G_5` 是多少？ 提示：反向工作。

*练习3.9* 假设 :math:`\gamma=0.9` 并且奖励序列是 :math:`R_1=2`，接着是无限序列的7s。
:math:`G_1` 和 :math:`G_0` 是什么？

*练习3.10* 证明（3.10）中的第二个等式。


3.4 情节和持续任务的统一符号
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

在上一节中，我们描述了两种强化学习任务，其中一种是个体-环境交互自然地分解为一系列单独的情节（情节任务），而另一种则不是（连续任务）。
前一种情况在数学上更容易，因为每个动作仅影响在情节期间随后收到的有限数量的奖励。
在本书中，我们有时会考虑一种问题，有时候会考虑另一种问题，但通常都会考虑。
因此，建立一种能够让我们同时准确地谈论这两种情况的符号是有用的。

准确地描述情节性任务需要一些额外的符号。我们需要考虑一系列情节，而不是一个长序列的时间步骤，每个情节都由有限的时间步骤序列组成。
我们从零开始重新编号每个情节的时间步长。因此，我们不仅要参考时间 :math:`t` 的状态表示  :math:`S_{t}`，
而且参考在情节 :math:`i` 和时间 :math:`t` 的状态表示 :math:`S_{t,i}`
（同样地，对于 :math:`A_{t,i}`，:math:`R_{t,i}`，:math:`\pi_{t,i}`，
:math:`T_{i}` 等符号意义相似）。
然而，事实证明，当我们讨论情节任务时，我们几乎从不必区分不同的情节。
我们几乎总是在考虑一个特定的情节，或者陈述对所有情节都适用的东西。
因此，在实践中，我们几乎总是通过省略情节编号的明确引用来使用符号。
也就是说，我们写 :math:`S_{t}` 来引指 :math:`S_{t,i}` 等等。

我们需要另一个约定来获得涵盖情节和持续任务的单一符号。
在一种情况下（3.7），我们将收益定义为有限数量的项的和，而在另一种情况下，将收益定义为无限数量的项（3.8）。
这两个可以通过考虑情节终止来统一，即进入一个特殊的吸收状态，该状态仅转换为自身并且仅产生零奖励。
例如，考虑状态转换图：

.. figure:: images/state_transition_diagram.png
   :alt: state transition diagram

   state transition diagram

这里实心方块表示对应于情节结束的特殊吸收状态。从 :math:`S_{0}` 开始，我们得到奖励序列+1，+1，+1，0，0，0，...。
总结这些，我们得到相同的回报，无论我们是否在前 :math:`T` 个奖励求和（这里 :math:`T=3`）还是在整个无限序列上求和。
即使我们引入衰减因子这仍然成立。因此，我们可以根据（3.8），使用在不需要时忽略情节编号的惯例来定义回报，
并且包括如果总和仍然被定义时 :math:`\gamma = 1` 的可能性（例如，因为所有情节终止）。或者，我们也可以写回报如下

.. math::
    :label: 3.11

    G_t \doteq \sum_{k=t+1}^{T} \gamma^{k-t-1} R_t

包括 :math:`T = \infty` 或 :math:`\gamma = 1` （但不能同时存在）的可能性。
我们在本书的剩余部分中使用这些约定来简化符号，并表达情节和持续任务之间的近乎相似。
（之后，在第10章中，我们将介绍一个持续未衰减的形式。）

3.5 策略和价值函数
^^^^^^^^^^^^^^^^^^^^^^

几乎所有的强化学习算法都涉及估计状态（或状态-动作对）的 *价值函数*，
它们估计个体在给定状态下的 *好坏程度* （或者在给定状态下执行给定动作的程度有多好）。
这里的“有多好”的概念是根据未来的奖励来定义的，或者准确的的说是预期回报方面。
当然，个体未来可能获得的回报取决于它将采取的行动。因此，价值函数是根据特定的行为方式来定义的，称为策略。

形式上，*策略* 是从状态到选择每个可能动作的概率的映射。如果个体在时间 :math:`t` 遵循策略 :math:`\pi`，
则 :math:`\pi(a|s)` 是如果 :math:`S_t=s`，则 :math:`A_t=a` 的概率。
像 :math:`p` 一样，:math:`\pi` 是一个普通的函数；
:math:`\pi(a|s)` 中间的“|”仅提醒它为每个 :math:`s\in \mathcal{S}`
定义了 :math:`a\in \mathcal{A}(s)` 的概率分布。
强化学习方法指定了个体的策略如何因其经验结果而变化。

*练习3.11* 如果当前状态为 :math:`S_t`，并且根据随机策略 :math:`\pi` 选择动作，
则对于 :math:`\pi` 和四参数函数 :math:`p` （3.2），:math:`R_{t+1}` 的期望是多少？

在状态 :math:`s` 下，策略 :math:`\pi` 下的 *价值函数* 表示为 :math:`v_\pi(s)` ，
是从 :math:`s` 开始并且之后遵循策略 :math:`\pi` 的预期收益。
对于 **MDPs**，我们可以正式将 :math:`v_\pi(s)` 定义为

.. math::
    :label: 3.12

    v_\pi(s) \doteq \mathbb{E}_\pi\left[G_t|S_t=s\right]
    = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t=s\right]，对所有 s\in \mathbb{S}

其中 :math:`\mathbb{E}[\dot]` 表示个体遵循策略 :math:`\pi` 的随机变量的期望值， :math:`t` 是任意的时间步长。
请注意，如果有终止状态的话，其值一直为0。我们称函数 :math:`v_\pi` 是 *策略* :math:`\pi` *的状态—价值函数*。


同样，我们在政策⇡下定义采取行动a的价值，表示为q⇡（s，a），作为从s开始的预期收益，采取行动a，然后遵循政策⇡：

同样，我们定义在策略 :math:`\pi`，状态 :math:`s` 下采取动作 :math:`a` 的价值，
表示为 :math:`q_\pi(s,a)`，作为从 :math:`s` 开始的，采取行动` a :math:`，
此后遵循策略 :math:`\pi` 的预期回报：

.. math::
    :label: 3.13

    q_\pi(s,a) \doteq \mathbb{E}_\pi\left[G_t|S_t=s,A_t=a\right]
    = \mathbb{E}_\pi\left[\sum^{\infty}_{k=0}\gamma^kR_{t+k+1}|S_t=s,A_t=a\right]

我们称 :math:`q_\pi` 为策略 :math:`\pi` 的动作值函数。

*练习3.12* 用 :math:`q_\pi` 和 :math:`\pi` 给出 :math:`v_\pi` 的等式。
*练习3.13* 根据 :math:`v_\pi` 和四参数 :math:`p` 给出 :math:`q_\pi` 的等式。

价值函数 :math:`v_\pi` 和 :math:`q_\pi` 可以根据经验估计。
例如，如果个体遵循策略 :math:`\pi`并且对于遇到的每个状态保持平均值，
那么该状态之后的实际返回值将收敛到状态价值 :math:`v+\pi(s)`，作为遇到的状态的次数接近无穷大。
如果为每个状态采取的每项行动保留单独的平均值，那么这些平均值将同样收敛于行动价值 :math:`q_\pi(s,a)`。
我们称这种估计方法为 *蒙特卡罗方法*，因为它们涉及对实际收益的许多随机样本进行平均。
这些方法在第5章中介绍。当然，如果有很多状态，那么单独为每个状态保持单独的平均值可能是不切实际的。
相反，个体将必须维护 :math:`v_\pi` 和 :math:`q_\pi` 作为参数化函数（参数少于状态），并调整参数以更好地匹配观察到的返回。
这也可以产生准确的估计，尽管很大程度上取决于参数化函数逼近器的性质。这些可能性在本书的第二部分中讨论。

在强化学习和动态规划中使用的价值函数的基本属性是它们满足类似于我们已经为返回建立的递归关系（3.9）。
对于任何策略 :math:`\pi` 和任何状态 :math:`s`，:math:`s` 的值与其可能的后继状态的值之间保持以下一致性条件：

.. math::
    :label: 3.14

    \begin{align*}
    v_\pi(s) &\doteq \mathbb{E}_\pi[G_t|S_t=s] \\
    &= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s] (由 (3.9)) \\
    &= \sum_a\pi(a|s) \sum_{s^\prime}\sum_r p(s^\prime,r|s,a) \left[r+\gamma\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s^\prime]\right] \\
    &= \sum_a\pi(a|s) \sum_{s^\prime,r}p(s^\prime,r|s,a)[r+\gamma v_\pi(s^\prime)], 对所有 s\in\mathcal{S}
    \end{align*}

其中隐含的动作 :math:`a` 取自集合 :math:`\mathcal{A}(s)`，
下一个状态 :math:`s^\prime` 取自集合 :math:`\mathcal{S}`
（或者在情节问题的情况下取自 :math:`\mathcal{S}+`），
并且奖励 :math:`r` 取自集合 :math:`\mathcal{R}`。
注意，在最后的等式中我们如何合并两个和，一个在 :math:`s^\prime` 的所有值上，
另一个在 :math:`r` 的所有值上，合并为所有可能值的一个和。
我们经常使用这种合并的和来简化公式。请注意最终表达式如何轻松作为期望值读取。
它实际上是三个变量 :math:`a`，:math:`s^\prime` 和 :math:`r` 的所有值的总和。
对于每个三元组，我们计算其概率 :math:`\pi(a|s)p(s^\prime,r|s,a)`，
用该概率对括号中的数量进行加权，然后对所有可能性求和得到预期值。




.. [1]
   我们使用术语个体，环境和动作，而不是工程师术语控制器，受控系统（或工厂）和控制信号，因为它们对更广泛的受众有意义。

.. [2]
   我们将注意力限制在离散时间以使事情尽可能简单，即使许多想法可以延伸到连续时间情况
   （例如，参见Bertsekas和Tsitsiklis，1996；Werbos，1992；Doya，1996）。

.. [3]
   我们使用 :math:`R_{t+1}` 而不是 :math:`R_{t}` 来表示归因于 :math:`A_{t}` 的奖励，
   因为它强调下一个奖励和下一个状态 :math:`R_{t+1}` 和 :math:`S_{t+1}` \ 共同确定。
   不幸的是，这两种惯例在文献中都被广泛使用。

.. [4]
   更好的方式是传授这种先验知识是最初的策略或价值功能，或对这些的影响。
   参见Lin（1992），Maclin和Shavlik（1994）和Clouse（1996）。

.. [5]
    传授这种先验知识的更好的地方是初始策略或初始价值功能，或影响这些。

.. [6]
    第17.4节进一步探讨了设计有效奖励信号的问题。

.. [7]
    情节有时在文献中被称为“试验”。
